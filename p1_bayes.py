# -*- coding: utf-8 -*-
"""P1.Bayes

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qc4LlvlFrCMjvfSgT5gPFIds4ywFqniy
"""

!pip -q install streamlit==1.* pyngrok==7.*
!pip install streamlit
!pip install -q "pymc>=5" "arviz>=0.16" "pytensor"

from google.colab import drive
drive.mount('/content/drive')

from google.colab import userdata
NGROK_TOKEN = userdata.get('NGROCK')

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# # matar processos antigos (sem erro se nÃ£o existir)
# pkill -f "streamlit run" || true
# pkill -f ngrok || true
# 
# # instalar dependÃªncias (idempotente)
# pip -q install streamlit pyngrok psycopg2-binary SQLAlchemy pandas numpy scikit-learn plotly arviz pymc xlrd
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/app.py
# # app mÃ­nimo sÃ³ pra testar. Troque pelo seu app completo quando quiser.
# import streamlit as st
# st.set_page_config(page_title="Hello", layout="wide")
# st.title("App Streamlit OK no Colab ðŸš€")
# st.write("Se vocÃª estÃ¡ vendo isso, o Streamlit estÃ¡ rodando certinho.")
#

# ==== 0) Instalar dependÃªncias (Colab) ====
!pip -q install -U streamlit==1.* pyngrok==7.* >/dev/null

# ==== 1) Imports e config ====
import os, sys, time, socket, subprocess, threading, queue
from pyngrok import ngrok

# -------- CONFIG --------
APP_PATH = "/content/app.py"              # <â€” corrigido
PREFERRED_PORTS = list(range(8501, 8511)) # tenta 8501..8510

# Se o app nÃ£o existir, criamos um mÃ­nimo sÃ³ pra testar
if not os.path.exists(APP_PATH):
    with open(APP_PATH, "w", encoding="utf-8") as f:
        f.write(
            "import streamlit as st\n"
            "st.set_page_config(page_title='Colab + ngrok')\n"
            "st.title('âœ… Streamlit no Colab via ngrok')\n"
            "st.write('Se vocÃª estÃ¡ vendo isto, deu certo!')\n"
        )
    print(f"ðŸ“„ Criado app mÃ­nimo em {APP_PATH}")

# ==== 2) Token (mantendo sua chave NGROCK) ====
NGROK_TOKEN = None
try:
    from google.colab import userdata
    # Mantendo exatamente a sua chave:
    NGROK_TOKEN = userdata.get('NGROCK')
except Exception:
    pass

# fallback: variÃ¡vel de ambiente (se quiser exportar NGROK_TOKEN no runtime)
if not NGROK_TOKEN:
    NGROK_TOKEN = os.environ.get("NGROK_TOKEN")

# Ãºltimo fallback: cole manualmente (se quiser)
# if not NGROK_TOKEN:
#     NGROK_TOKEN = "COLE_SEU_TOKEN_AQUI"

if not NGROK_TOKEN:
    raise RuntimeError("âŒ Defina o token do ngrok (userdata['NGROCK'] ou env NGROK_TOKEN).")

# Ambiente do Streamlit
os.environ["STREAMLIT_SERVER_HEADLESS"] = "true"
os.environ["STREAMLIT_BROWSER_GATHER_USAGE_STATS"] = "false"

# ==== 3) Helpers ====
def port_is_free(port:int)->bool:
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(0.25)
        return s.connect_ex(("127.0.0.1", port)) != 0

def start_streamlit(app_path:str, port:int)->subprocess.Popen:
    # Usamos python -m streamlit para evitar PATHs estranhos do Colab
    return subprocess.Popen(
        [sys.executable, "-m", "streamlit", "run", app_path,
         "--server.address", "0.0.0.0",
         "--server.port", str(port)],
        stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1
    )

def follow_until_ready(proc:subprocess.Popen, timeout:int=120)->bool:
    q = queue.Queue()
    def reader():
        for line in proc.stdout:
            q.put(line)
    t = threading.Thread(target=reader, daemon=True)
    t.start()

    start = time.time()
    ready = False
    while time.time() - start < timeout:
        while not q.empty():
            line = q.get()
            # Mostramos logs do Streamlit na cÃ©lula
            sys.stdout.write(line)
            # Sinais que o Streamlit estÃ¡ de pÃ©
            if ("Network URL:" in line) or ("Now running at" in line) or ("You can now view your Streamlit app" in line):
                ready = True
        if ready:
            return True
        if proc.poll() is not None:
            return False
        time.sleep(0.15)
    return False

# ==== 4) Limpar tÃºneis antigos e setar token ====
try:
    for t in ngrok.get_tunnels():
        ngrok.disconnect(t.public_url)
except Exception:
    pass

ngrok.set_auth_token(NGROK_TOKEN)

# ==== 5) Subir Streamlit na primeira porta livre ====
proc = None
chosen_port = None
for p in PREFERRED_PORTS:
    if not port_is_free(p):
        continue
    print(f"âž¡ï¸ Tentando iniciar Streamlit em 0.0.0.0:{p} ...")
    proc = start_streamlit(APP_PATH, p)
    if follow_until_ready(proc, timeout=120):
        chosen_port = p
        print(f"âœ… Streamlit ativo em 0.0.0.0:{p}")
        break
    else:
        try: proc.terminate()
        except Exception: pass
        print(f"âš ï¸ Falhou em {p}. Tentando prÃ³xima porta...")

if not chosen_port:
    raise RuntimeError("âŒ NÃ£o consegui iniciar o Streamlit (veja logs acima).")

# ==== 6) Criar tÃºnel ngrok e mostrar o LINK ====
# ObservaÃ§Ã£o: usar proto="http" e passar a porta numÃ©rica Ã© o mais estÃ¡vel no Colab
public_url = ngrok.connect(chosen_port, proto="http").public_url

print("\n" + "="*72)
print("ðŸŒ Seu app estÃ¡ disponÃ­vel em:")
print(public_url)
print("="*72)

# (Opcional) Exibir tÃºneis ativos
try:
    tunnels = ngrok.get_tunnels()
    print("TÃºneis ativos:", [t.public_url for t in tunnels])
except Exception:
    pass

import psycopg2

# ParÃ¢metros de conexÃ£o
server = "bigdata.dataiesb.com"
port = 5432
user = "data_iesb"
password = "iesb"
database = "iesb"

# Conectando ao banco
conn = psycopg2.connect(
    host=server,
    port=port,
    user=user,
    password=password,
    dbname=database
)

# Criando um cursor
cur = conn.cursor()

# Executando um comando SQL simples
cur.execute("SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';")

# Pegando os resultados
tabelas = cur.fetchall()
for tabela in tabelas:
    print(tabela[0])

# Fechando tudo
cur.close()
conn.close()

import pandas as pd
from sqlalchemy import create_engine

# Dados de conexÃ£o
server = "bigdata.dataiesb.com"
port = 5432
user = "data_iesb"
password = "iesb"
database = "iesb"

# Cria o engine SQLAlchemy
engine = create_engine(
    f"postgresql+psycopg2://{user}:{password}@{server}:{port}/{database}"
)

# LÃª a tabela 'municipio' do schema 'public'
df = pd.read_sql_table(
    table_name="pib_municipios",
    con=engine,
    schema="public"
)

# Visualiza as 5 primeiras linhas
print(df.head())

df

!pip -q install "pymc>=5" "arviz>=0.17" "pytensor>=2"

!pip -q install SQLAlchemy psycopg2-binary scikit-learn pymc arviz xlrd
!python train_forecast.py --use_bayes  \
  --out_dir "/content/drive/MyDrive/PIB_Forecast" \
  --ibge_xls_path "/content/drive/MyDrive/Bases de Dados/RELATORIO_DTB_BRASIL_2024_MUNICIPIOS.xls" \
  --max_lags 2 --min_train 5
!pip -q install streamlit plotly scikit-learn

"""# **RODANDO OS MODELOS FORA DO STREAMLIT**"""

# Commented out IPython magic to ensure Python compatibility.
# %pip -q install -U "xlrd>=2.0.1"  # para .xls
# se o arquivo for .xlsx, instale openpyxl e use esse engine:
# %pip -q install openpyxl

!pip -q install "pymc>=5" "arviz>=0.17"
# opcional (normalmente vem como dependÃªncia do PyMC)
!pip -q install "pytensor>=2"
# %pip -q install "pymc==5.*" "arviz>=0.15" "psycopg2-binary<3.2" SQLAlchemy "xlrd==1.2.0"

# %pip -q install -U "pymc==5.*" arviz "psycopg2-binary<3.2" SQLAlchemy "xlrd>=2.0.1" openpyxl

"""***SANITIZAÃ‡ÃƒO***"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/PIB_Forecast/common_setor_utils.py
# # -*- coding: utf-8 -*-
# import os, unicodedata
# import numpy as np
# import pandas as pd
# from sqlalchemy import create_engine, text
# 
# from sklearn.pipeline import Pipeline
# from sklearn.compose import ColumnTransformer
# from sklearn.preprocessing import StandardScaler
# from sklearn.impute import SimpleImputer
# 
# # ==================== SanitizaÃ§Ã£o de colunas ====================
# _NUMPY_RESERVED = {
#     "sum","mean","std","var","min","max","median","average","clip","round","log","log1p","exp",
#     "sin","cos","tan","arcsin","arccos","arctan","sqrt","abs","sign","where","all","any","prod",
#     "cumsum","cumprod","argsort","argmin","argmax","astype","dtype","shape","size","ndim","ndarray",
#     "array","matrix","transpose","dot","einsum","unique","isnan","isfinite","isinf","conj","imag","real",
#     "nan","inf","pi","e","load","save","fromfile","tofile"
# }
# import builtins as _py_builtins
# _BUILTIN_RESERVED = {n for n in dir(_py_builtins)}
# _RESERVED = {s.lower() for s in (_NUMPY_RESERVED | _BUILTIN_RESERVED)}
# 
# def sanitize_columns(df: pd.DataFrame) -> pd.DataFrame:
#     new_cols = []
#     for c in df.columns:
#         base = str(c).strip().lower().replace(" ", "_")
#         base = "".join(ch if ch.isalnum() or ch == "_" else "_" for ch in base)
#         if base in _RESERVED:
#             base = f"{base}_col"
#         new_cols.append(base)
#     out = df.copy()
#     out.columns = new_cols
#     return out
# 
# # ==================== Aliases ====================
# ALIASES = {
#     "ano_pib": ["ano_pib","no_pib","ano","anopib","ano_referencia","ano_ref"],
#     "cod_mun": ["cod_mun","codigo_municipio_dv","codigo_municipio","cd_municipio","co_municipio",
#                 "co_mun","id_municipio","cod_ibge","codigo_ibge","codigo_mun","cod_municipio"],
#     "vl_agropecuaria": ["vl_agropecuaria","agropecuaria","valor_agropecuaria"],
#     "vl_industria":    ["vl_industria","industria","valor_industria"],
#     "vl_servicos":     ["vl_servicos","servicos","valor_servicos"],
#     "vl_administracao":["vl_administracao","administracao","adm_publica","valor_administracao"],
#     "vl_subsidios":    ["vl_subsidios","subsidios","valor_subsidios"],
# }
# SETORES_CANON = ["vl_agropecuaria","vl_industria","vl_servicos","vl_administracao","vl_subsidios"]
# 
# def _choose(cols, candidates):
#     for c in candidates:
#         if c in cols:
#             return c
#     return None
# 
# def ensure_dir(p):
#     os.makedirs(p, exist_ok=True)
#     return p
# 
# # ==================== DB ====================
# def get_engine_from_env(args):
#     import os
#     host = args.db_host or os.getenv("IESB_HOST","bigdata.dataiesb.com")
#     port = int(args.db_port or os.getenv("IESB_PORT","5432"))
#     db   = args.db_name or os.getenv("IESB_DB","iesb")
#     usr  = args.db_user or os.getenv("IESB_USER","data_iesb")
#     pwd  = args.db_pwd  or os.getenv("IESB_PWD","iesb")
#     sch  = args.db_schema or os.getenv("IESB_SCHEMA","public")
#     url  = f"postgresql+psycopg2://{usr}:{pwd}@{host}:{port}/{db}"
#     return create_engine(url, connect_args={"options": f"-csearch_path={sch}"})
# 
# def load_raw_table(eng, table):
#     df = pd.read_sql(text(f"SELECT * FROM {table}"), con=eng)
#     return sanitize_columns(df)
# 
# # ==================== NormalizaÃ§Ã£o / Melt ====================
# def normalize_keys(df):
#     cols = df.columns
#     c_ano = _choose(cols, ALIASES["ano_pib"]); c_cod = _choose(cols, ALIASES["cod_mun"])
#     if not c_ano or not c_cod:
#         raise ValueError(f"faltou ano/cod_mun. Colunas: {list(cols)}")
# 
#     df_keys = df[[c_ano, c_cod]].copy().rename(columns={c_ano:"ano_pib", c_cod:"cod_mun"})
#     df_keys["ano_pib"] = pd.to_numeric(df_keys["ano_pib"], errors="coerce").astype("Int64")
#     df_keys = df_keys.dropna(subset=["ano_pib","cod_mun"]).copy()
#     df_keys["ano_pib"] = df_keys["ano_pib"].astype(int)
#     df_keys["cod_mun"] = df_keys["cod_mun"].astype(str).str.replace(r"\.0$","",regex=True).str.zfill(7)
#     df_keys["nome_municipio"] = df_keys["cod_mun"]
# 
#     keep_cols = [c for c in df.columns if c not in df_keys.columns]
#     return df_keys.merge(df[keep_cols], left_index=True, right_index=True, how="left")
# 
# def find_sector_columns(df):
#     cols = df.columns
#     found = {}
#     for canon in SETORES_CANON:
#         c = _choose(cols, ALIASES[canon])
#         if c:
#             found[canon] = c
#     return found
# 
# def melt_setores(df):
#     mapping = find_sector_columns(df)
#     if not mapping:
#         raise ValueError("sem colunas de setores mapeadas (apÃ³s sanitize)")
#     value_cols = list(mapping.values())
#     id_vars = [c for c in ["ano_pib","cod_mun","nome_municipio"] if c in df.columns]
# 
#     df_long = df[id_vars + value_cols].melt(
#         id_vars=id_vars, value_vars=value_cols, var_name="setor", value_name="valor"
#     )
#     inv = {v:k for k,v in mapping.items()}
#     df_long["setor"] = df_long["setor"].map(inv).fillna(df_long["setor"])
# 
#     df_total = (
#         df_long.groupby(["ano_pib","cod_mun"], as_index=False)["valor"]
#         .sum()
#         .rename(columns={"valor":"total_setores"})
#     )
#     df_long = df_long.merge(df_total, on=["ano_pib","cod_mun"], how="left")
#     df_long["percentual"] = np.where(
#         df_long["total_setores"]>0, 100.0*df_long["valor"]/df_long["total_setores"], np.nan
#     )
#     return df_long
# 
# # ==================== IBGE robusto ====================
# def _strip_accents(s: str) -> str:
#     if not isinstance(s, str):
#         s = str(s)
#     return ''.join(ch for ch in unicodedata.normalize('NFKD', s) if not unicodedata.combining(ch))
# 
# def _normalize_cols(df: pd.DataFrame) -> pd.DataFrame:
#     cols = []
#     for c in df.columns:
#         base = _strip_accents(str(c)).lower().strip().replace(" ", "_")
#         base = "".join(ch if ch.isalnum() or ch == "_" else "_" for ch in base)
#         cols.append(base)
#     out = df.copy()
#     out.columns = cols
#     return out
# 
# def try_merge_ibge(df, xls_path):
#     if not xls_path or not os.path.exists(xls_path):
#         print("[INFO] sem IBGE XLS/XLSX â€” pulando")
#         return df
# 
#     ext = os.path.splitext(xls_path)[1].lower()
#     engine = "openpyxl" if ext == ".xlsx" else "xlrd"
# 
#     try:
#         x = pd.read_excel(xls_path, header=6, engine=engine)
#     except Exception:
#         x = pd.read_excel(xls_path, header=0, engine=engine)
# 
#     x = _normalize_cols(x)
# 
#     code_candidates = [
#         "codigo_municipio_completo","codigo_municipio","cod_municipio",
#         "codigo_municipio_ibge","codigo_ibge","codigo_mun","cod_ibge"
#     ]
#     name_candidates = [
#         "nome_municipio","nome_municipio_","nome_municipio_ibge","nome_do_municipio","nome"
#     ]
#     code_col = next((c for c in code_candidates if c in x.columns), None)
#     name_col = next((c for c in name_candidates if c in x.columns), None)
# 
#     if not code_col or not name_col:
#         try:
#             x2 = pd.read_excel(xls_path, header=None, engine=engine)
#             header_row = None
#             for i in range(min(20, len(x2))):
#                 row_vals = [_strip_accents(str(v)).lower().strip() for v in x2.iloc[i].values]
#                 if ("codigo municipio completo" in row_vals) or ("codigo_municipio_completo" in row_vals):
#                     header_row = i; break
#             if header_row is not None:
#                 x = pd.read_excel(xls_path, header=header_row, engine=engine)
#                 x = _normalize_cols(x)
#                 code_col = next((c for c in code_candidates if c in x.columns), None)
#                 name_col = next((c for c in name_candidates if c in x.columns), None)
#         except Exception:
#             pass
# 
#     if not code_col or not name_col:
#         print(f"[WARN] IBGE: nÃ£o encontrei colunas de cÃ³digo/nome. Colunas: {list(x.columns)[:10]}...")
#         return df
# 
#     x = x[[code_col, name_col]].dropna()
#     x.rename(columns={code_col:"codigo_municipio_completo", name_col:"nome_municipio_ibge"}, inplace=True)
#     x["codigo_municipio_completo"] = (
#         x["codigo_municipio_completo"].astype(str).str.replace(r"\.0$","",regex=True).str.zfill(7)
#     )
# 
#     before = len(df)
#     df_merged = df.merge(x, left_on="cod_mun", right_on="codigo_municipio_completo", how="left")
#     df_merged["nome_municipio"] = df_merged.get("nome_municipio", df_merged["cod_mun"])
#     df_merged["nome_municipio"] = df_merged["nome_municipio_ibge"].fillna(df_merged["nome_municipio"])
#     matched = df_merged["nome_municipio_ibge"].notna().sum()
#     print(f"[INFO] IBGE merge: {matched}/{before} cÃ³digos casados ({matched/before:.1%}).")
# 
#     return df_merged.drop(columns=["codigo_municipio_completo","nome_municipio_ibge"], errors="ignore")
# 
# # ==================== PrÃ©-processamento comum ====================
# def build_numeric_preprocessor(feat_cols=None):
#     feat_cols = feat_cols or ["ano_pib","valor_log","percentual"]
#     preproc = ColumnTransformer([
#         ("num", Pipeline([
#             ("imp", SimpleImputer(strategy="median")),
#             ("scaler", StandardScaler())
#         ]), feat_cols)
#     ], remainder="drop")
#     return preproc, feat_cols
# 
# def prepare_long_for_classification(long_df):
#     dfc = long_df.copy()
#     dfc["valor"] = pd.to_numeric(dfc["valor"], errors="coerce").clip(lower=0)
#     dfc["percentual"] = pd.to_numeric(dfc["percentual"], errors="coerce")
#     dfc["valor_log"] = np.log1p(dfc["valor"])
#     dfc = dfc.replace([np.inf, -np.inf], np.nan)
#     X = dfc[["ano_pib","valor_log","percentual"]]
#     y = dfc["setor"].astype(str)
#     return X, y, dfc
#

"""**FREQUENTISTA**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/PIB_Forecast/common_setor_utils.py
# # -*- coding: utf-8 -*-
# import os, unicodedata
# import numpy as np
# import pandas as pd
# from sqlalchemy import create_engine, text
# 
# from sklearn.pipeline import Pipeline
# from sklearn.compose import ColumnTransformer
# from sklearn.preprocessing import StandardScaler
# from sklearn.impute import SimpleImputer
# 
# # ==================== SanitizaÃ§Ã£o de colunas ====================
# _NUMPY_RESERVED = {
#     "sum","mean","std","var","min","max","median","average","clip","round","log","log1p","exp",
#     "sin","cos","tan","arcsin","arccos","arctan","sqrt","abs","sign","where","all","any","prod",
#     "cumsum","cumprod","argsort","argmin","argmax","astype","dtype","shape","size","ndim","ndarray",
#     "array","matrix","transpose","dot","einsum","unique","isnan","isfinite","isinf","conj","imag","real",
#     "nan","inf","pi","e","load","save","fromfile","tofile"
# }
# import builtins as _py_builtins
# _BUILTIN_RESERVED = {n for n in dir(_py_builtins)}
# _RESERVED = {s.lower() for s in (_NUMPY_RESERVED | _BUILTIN_RESERVED)}
# 
# def sanitize_columns(df: pd.DataFrame) -> pd.DataFrame:
#     new_cols = []
#     for c in df.columns:
#         base = str(c).strip().lower().replace(" ", "_")
#         base = "".join(ch if ch.isalnum() or ch == "_" else "_" for ch in base)
#         if base in _RESERVED:
#             base = f"{base}_col"
#         new_cols.append(base)
#     out = df.copy()
#     out.columns = new_cols
#     return out
# 
# # ==================== Aliases ====================
# ALIASES = {
#     "ano_pib": ["ano_pib","no_pib","ano","anopib","ano_referencia","ano_ref"],
#     "cod_mun": ["cod_mun","codigo_municipio_dv","codigo_municipio","cd_municipio","co_municipio",
#                 "co_mun","id_municipio","cod_ibge","codigo_ibge","codigo_mun","cod_municipio"],
#     "vl_agropecuaria": ["vl_agropecuaria","agropecuaria","valor_agropecuaria"],
#     "vl_industria":    ["vl_industria","industria","valor_industria"],
#     "vl_servicos":     ["vl_servicos","servicos","valor_servicos"],
#     "vl_administracao":["vl_administracao","administracao","adm_publica","valor_administracao"],
#     "vl_subsidios":    ["vl_subsidios","subsidios","valor_subsidios"],
# }
# SETORES_CANON = ["vl_agropecuaria","vl_industria","vl_servicos","vl_administracao","vl_subsidios"]
# 
# def _choose(cols, candidates):
#     for c in candidates:
#         if c in cols:
#             return c
#     return None
# 
# def ensure_dir(p):
#     os.makedirs(p, exist_ok=True)
#     return p
# 
# # ==================== DB ====================
# def get_engine_from_env(args):
#     import os
#     host = args.db_host or os.getenv("IESB_HOST","bigdata.dataiesb.com")
#     port = int(args.db_port or os.getenv("IESB_PORT","5432"))
#     db   = args.db_name or os.getenv("IESB_DB","iesb")
#     usr  = args.db_user or os.getenv("IESB_USER","data_iesb")
#     pwd  = args.db_pwd  or os.getenv("IESB_PWD","iesb")
#     sch  = args.db_schema or os.getenv("IESB_SCHEMA","public")
#     url  = f"postgresql+psycopg2://{usr}:{pwd}@{host}:{port}/{db}"
#     return create_engine(url, connect_args={"options": f"-csearch_path={sch}"})
# 
# def load_raw_table(eng, table):
#     df = pd.read_sql(text(f"SELECT * FROM {table}"), con=eng)
#     return sanitize_columns(df)
# 
# # ==================== NormalizaÃ§Ã£o / Melt ====================
# def normalize_keys(df):
#     cols = df.columns
#     c_ano = _choose(cols, ALIASES["ano_pib"]); c_cod = _choose(cols, ALIASES["cod_mun"])
#     if not c_ano or not c_cod:
#         raise ValueError(f"faltou ano/cod_mun. Colunas: {list(cols)}")
# 
#     df_keys = df[[c_ano, c_cod]].copy().rename(columns={c_ano:"ano_pib", c_cod:"cod_mun"})
#     df_keys["ano_pib"] = pd.to_numeric(df_keys["ano_pib"], errors="coerce").astype("Int64")
#     df_keys = df_keys.dropna(subset=["ano_pib","cod_mun"]).copy()
#     df_keys["ano_pib"] = df_keys["ano_pib"].astype(int)
#     df_keys["cod_mun"] = df_keys["cod_mun"].astype(str).str.replace(r"\.0$","",regex=True).str.zfill(7)
#     df_keys["nome_municipio"] = df_keys["cod_mun"]
# 
#     keep_cols = [c for c in df.columns if c not in df_keys.columns]
#     return df_keys.merge(df[keep_cols], left_index=True, right_index=True, how="left")
# 
# def find_sector_columns(df):
#     cols = df.columns
#     found = {}
#     for canon in SETORES_CANON:
#         c = _choose(cols, ALIASES[canon])
#         if c:
#             found[canon] = c
#     return found
# 
# def melt_setores(df):
#     mapping = find_sector_columns(df)
#     if not mapping:
#         raise ValueError("sem colunas de setores mapeadas (apÃ³s sanitize)")
#     value_cols = list(mapping.values())
#     id_vars = [c for c in ["ano_pib","cod_mun","nome_municipio"] if c in df.columns]
# 
#     df_long = df[id_vars + value_cols].melt(
#         id_vars=id_vars, value_vars=value_cols, var_name="setor", value_name="valor"
#     )
#     inv = {v:k for k,v in mapping.items()}
#     df_long["setor"] = df_long["setor"].map(inv).fillna(df_long["setor"])
# 
#     df_total = (
#         df_long.groupby(["ano_pib","cod_mun"], as_index=False)["valor"]
#         .sum()
#         .rename(columns={"valor":"total_setores"})
#     )
#     df_long = df_long.merge(df_total, on=["ano_pib","cod_mun"], how="left")
#     df_long["percentual"] = np.where(
#         df_long["total_setores"]>0, 100.0*df_long["valor"]/df_long["total_setores"], np.nan
#     )
#     return df_long
# 
# # ==================== IBGE robusto ====================
# def _strip_accents(s: str) -> str:
#     if not isinstance(s, str):
#         s = str(s)
#     return ''.join(ch for ch in unicodedata.normalize('NFKD', s) if not unicodedata.combining(ch))
# 
# def _normalize_cols(df: pd.DataFrame) -> pd.DataFrame:
#     cols = []
#     for c in df.columns:
#         base = _strip_accents(str(c)).lower().strip().replace(" ", "_")
#         base = "".join(ch if ch.isalnum() or ch == "_" else "_" for ch in base)
#         cols.append(base)
#     out = df.copy()
#     out.columns = cols
#     return out
# 
# def try_merge_ibge(df, xls_path):
#     if not xls_path or not os.path.exists(xls_path):
#         print("[INFO] sem IBGE XLS/XLSX â€” pulando")
#         return df
# 
#     ext = os.path.splitext(xls_path)[1].lower()
#     engine = "openpyxl" if ext == ".xlsx" else "xlrd"
# 
#     try:
#         x = pd.read_excel(xls_path, header=6, engine=engine)
#     except Exception:
#         x = pd.read_excel(xls_path, header=0, engine=engine)
# 
#     x = _normalize_cols(x)
# 
#     code_candidates = [
#         "codigo_municipio_completo","codigo_municipio","cod_municipio",
#         "codigo_municipio_ibge","codigo_ibge","codigo_mun","cod_ibge"
#     ]
#     name_candidates = [
#         "nome_municipio","nome_municipio_","nome_municipio_ibge","nome_do_municipio","nome"
#     ]
#     code_col = next((c for c in code_candidates if c in x.columns), None)
#     name_col = next((c for c in name_candidates if c in x.columns), None)
# 
#     if not code_col or not name_col:
#         try:
#             x2 = pd.read_excel(xls_path, header=None, engine=engine)
#             header_row = None
#             for i in range(min(20, len(x2))):
#                 row_vals = [_strip_accents(str(v)).lower().strip() for v in x2.iloc[i].values]
#                 if ("codigo municipio completo" in row_vals) or ("codigo_municipio_completo" in row_vals):
#                     header_row = i; break
#             if header_row is not None:
#                 x = pd.read_excel(xls_path, header=header_row, engine=engine)
#                 x = _normalize_cols(x)
#                 code_col = next((c for c in code_candidates if c in x.columns), None)
#                 name_col = next((c for c in name_candidates if c in x.columns), None)
#         except Exception:
#             pass
# 
#     if not code_col or not name_col:
#         print(f"[WARN] IBGE: nÃ£o encontrei colunas de cÃ³digo/nome. Colunas: {list(x.columns)[:10]}...")
#         return df
# 
#     x = x[[code_col, name_col]].dropna()
#     x.rename(columns={code_col:"codigo_municipio_completo", name_col:"nome_municipio_ibge"}, inplace=True)
#     x["codigo_municipio_completo"] = (
#         x["codigo_municipio_completo"].astype(str).str.replace(r"\.0$","",regex=True).str.zfill(7)
#     )
# 
#     before = len(df)
#     df_merged = df.merge(x, left_on="cod_mun", right_on="codigo_municipio_completo", how="left")
#     df_merged["nome_municipio"] = df_merged.get("nome_municipio", df_merged["cod_mun"])
#     df_merged["nome_municipio"] = df_merged["nome_municipio_ibge"].fillna(df_merged["nome_municipio"])
#     matched = df_merged["nome_municipio_ibge"].notna().sum()
#     print(f"[INFO] IBGE merge: {matched}/{before} cÃ³digos casados ({matched/before:.1%}).")
# 
#     return df_merged.drop(columns=["codigo_municipio_completo","nome_municipio_ibge"], errors="ignore")
# 
# # ==================== PrÃ©-processamento comum ====================
# def build_numeric_preprocessor(feat_cols=None):
#     # inclui features relativas por municÃ­pio-ano (rank e zscore)
#     feat_cols = feat_cols or ["ano_pib","valor_log","percentual","rank_setor_no_mun","zscore_no_mun"]
#     preproc = ColumnTransformer([
#         ("num", Pipeline([
#             ("imp", SimpleImputer(strategy="median")),
#             ("scaler", StandardScaler())
#         ]), feat_cols)
#     ], remainder="drop")
#     return preproc, feat_cols
# 
# def prepare_long_for_classification(long_df):
#     dfc = long_df.copy()
#     dfc["valor"] = pd.to_numeric(dfc["valor"], errors="coerce").clip(lower=0)
#     dfc["percentual"] = pd.to_numeric(dfc["percentual"], errors="coerce")
#     dfc["valor_log"] = np.log1p(dfc["valor"])
# 
#     # Sinais relativos por municÃ­pio-ano (100% numÃ©ricos)
#     grp = dfc.groupby(["ano_pib","cod_mun"], group_keys=False)
#     dfc["rank_setor_no_mun"] = grp["valor"].rank(ascending=False, method="dense")
#     dfc["zscore_no_mun"] = grp["valor"].transform(lambda s: (s - s.mean()) / (s.std(ddof=0) + 1e-9))
# 
#     dfc = dfc.replace([np.inf, -np.inf], np.nan)
# 
#     X = dfc[["ano_pib","valor_log","percentual","rank_setor_no_mun","zscore_no_mun"]]
#     y = dfc["setor"].astype(str)
#     return X, y, dfc
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/PIB_Forecast/common_setor_utils.py
# # -*- coding: utf-8 -*-
# import os, unicodedata
# import numpy as np
# import pandas as pd
# from sqlalchemy import create_engine, text
# 
# from sklearn.pipeline import Pipeline
# from sklearn.compose import ColumnTransformer
# from sklearn.preprocessing import StandardScaler
# from sklearn.impute import SimpleImputer
# import builtins as _py_builtins
# 
# # ==================== SanitizaÃ§Ã£o de colunas ====================
# _NUMPY_RESERVED = {
#     "sum","mean","std","var","min","max","median","average","clip","round","log","log1p","exp",
#     "sin","cos","tan","arcsin","arccos","arctan","sqrt","abs","sign","where","all","any","prod",
#     "cumsum","cumprod","argsort","argmin","argmax","astype","dtype","shape","size","ndim","ndarray",
#     "array","matrix","transpose","dot","einsum","unique","isnan","isfinite","isinf","conj","imag","real",
#     "nan","inf","pi","e","load","save","fromfile","tofile"
# }
# _BUILTIN_RESERVED = {n for n in dir(_py_builtins)}
# _RESERVED = {s.lower() for s in (_NUMPY_RESERVED | _BUILTIN_RESERVED)}
# 
# def sanitize_columns(df: pd.DataFrame) -> pd.DataFrame:
#     new_cols = []
#     for c in df.columns:
#         base = str(c).strip().lower().replace(" ", "_")
#         base = "".join(ch if ch.isalnum() or ch == "_" else "_" for ch in base)
#         if base in _RESERVED:
#             base = f"{base}_col"
#         new_cols.append(base)
#     out = df.copy()
#     out.columns = new_cols
#     return out
# 
# # ==================== Aliases ====================
# ALIASES = {
#     "ano_pib": ["ano_pib","no_pib","ano","anopib","ano_referencia","ano_ref"],
#     "cod_mun": ["cod_mun","codigo_municipio_dv","codigo_municipio","cd_municipio","co_municipio",
#                 "co_mun","id_municipio","cod_ibge","codigo_ibge","codigo_mun","cod_municipio"],
#     "vl_agropecuaria": ["vl_agropecuaria","agropecuaria","valor_agropecuaria"],
#     "vl_industria":    ["vl_industria","industria","valor_industria"],
#     "vl_servicos":     ["vl_servicos","servicos","valor_servicos"],
#     "vl_administracao":["vl_administracao","administracao","adm_publica","valor_administracao"],
#     "vl_subsidios":    ["vl_subsidios","subsidios","valor_subsidios"],
# }
# SETORES_CANON = ["vl_agropecuaria","vl_industria","vl_servicos","vl_administracao","vl_subsidios"]
# 
# def _choose(cols, candidates):
#     for c in candidates:
#         if c in cols:
#             return c
#     return None
# 
# def ensure_dir(p):
#     os.makedirs(p, exist_ok=True)
#     return p
# 
# # ==================== DB ====================
# def get_engine_from_env(args):
#     host = args.db_host or os.getenv("IESB_HOST","bigdata.dataiesb.com")
#     port = int(args.db_port or os.getenv("IESB_PORT","5432"))
#     db   = args.db_name or os.getenv("IESB_DB","iesb")
#     usr  = args.db_user or os.getenv("IESB_USER","data_iesb")
#     pwd  = args.db_pwd  or os.getenv("IESB_PWD","iesb")
#     sch  = args.db_schema or os.getenv("IESB_SCHEMA","public")
#     url  = f"postgresql+psycopg2://{usr}:{pwd}@{host}:{port}/{db}"
#     return create_engine(url, connect_args={"options": f"-csearch_path={sch}"})
# 
# def load_raw_table(eng, table):
#     df = pd.read_sql(text(f"SELECT * FROM {table}"), con=eng)
#     return sanitize_columns(df)
# 
# # ==================== NormalizaÃ§Ã£o / Melt ====================
# def normalize_keys(df):
#     cols = df.columns
#     c_ano = _choose(cols, ALIASES["ano_pib"]); c_cod = _choose(cols, ALIASES["cod_mun"])
#     if not c_ano or not c_cod:
#         raise ValueError(f"faltou ano/cod_mun. Colunas: {list(cols)}")
# 
#     df_keys = df[[c_ano, c_cod]].copy().rename(columns={c_ano:"ano_pib", c_cod:"cod_mun"})
#     df_keys["ano_pib"] = pd.to_numeric(df_keys["ano_pib"], errors="coerce").astype("Int64")
#     df_keys = df_keys.dropna(subset=["ano_pib","cod_mun"]).copy()
#     df_keys["ano_pib"] = df_keys["ano_pib"].astype(int)
#     df_keys["cod_mun"] = df_keys["cod_mun"].astype(str).str.replace(r"\.0$","",regex=True).str.zfill(7)
#     df_keys["nome_municipio"] = df_keys["cod_mun"]
# 
#     keep_cols = [c for c in df.columns if c not in df_keys.columns]
#     return df_keys.merge(df[keep_cols], left_index=True, right_index=True, how="left")
# 
# def find_sector_columns(df):
#     cols = df.columns
#     found = {}
#     for canon in SETORES_CANON:
#         c = _choose(cols, ALIASES[canon])
#         if c:
#             found[canon] = c
#     return found
# 
# def melt_setores(df):
#     mapping = find_sector_columns(df)
#     if not mapping:
#         raise ValueError("sem colunas de setores mapeadas (apÃ³s sanitize)")
#     value_cols = list(mapping.values())
#     id_vars = [c for c in ["ano_pib","cod_mun","nome_municipio"] if c in df.columns]
# 
#     df_long = df[id_vars + value_cols].melt(
#         id_vars=id_vars, value_vars=value_cols, var_name="setor", value_name="valor"
#     )
#     inv = {v:k for k,v in mapping.items()}
#     df_long["setor"] = df_long["setor"].map(inv).fillna(df_long["setor"])
# 
#     df_total = (
#         df_long.groupby(["ano_pib","cod_mun"], as_index=False)["valor"]
#         .sum()
#         .rename(columns={"valor":"total_setores"})
#     )
#     df_long = df_long.merge(df_total, on=["ano_pib","cod_mun"], how="left")
#     df_long["percentual"] = np.where(
#         df_long["total_setores"]>0, 100.0*df_long["valor"]/df_long["total_setores"], np.nan
#     )
#     return df_long
# 
# # ==================== IBGE robusto ====================
# def _strip_accents(s: str) -> str:
#     if not isinstance(s, str):
#         s = str(s)
#     return ''.join(ch for ch in unicodedata.normalize('NFKD', s) if not unicodedata.combining(ch))
# 
# def _normalize_cols(df: pd.DataFrame) -> pd.DataFrame:
#     cols = []
#     for c in df.columns:
#         base = _strip_accents(str(c)).lower().strip().replace(" ", "_")
#         base = "".join(ch if ch.isalnum() or ch == "_" else "_" for ch in base)
#         cols.append(base)
#     out = df.copy()
#     out.columns = cols
#     return out
# 
# def try_merge_ibge(df, xls_path):
#     if not xls_path or not os.path.exists(xls_path):
#         print("[INFO] sem IBGE XLS/XLSX â€” pulando")
#         return df
# 
#     ext = os.path.splitext(xls_path)[1].lower()
#     engine = "openpyxl" if ext == ".xlsx" else "xlrd"
# 
#     try:
#         x = pd.read_excel(xls_path, header=6, engine=engine)
#     except Exception:
#         x = pd.read_excel(xls_path, header=0, engine=engine)
# 
#     x = _normalize_cols(x)
# 
#     code_candidates = [
#         "codigo_municipio_completo","codigo_municipio","cod_municipio",
#         "codigo_municipio_ibge","codigo_ibge","codigo_mun","cod_ibge"
#     ]
#     name_candidates = [
#         "nome_municipio","nome_municipio_","nome_municipio_ibge","nome_do_municipio","nome"
#     ]
#     code_col = next((c for c in code_candidates if c in x.columns), None)
#     name_col = next((c for c in name_candidates if c in x.columns), None)
# 
#     if not code_col or not name_col:
#         try:
#             x2 = pd.read_excel(xls_path, header=None, engine=engine)
#             header_row = None
#             for i in range(min(20, len(x2))):
#                 row_vals = [_strip_accents(str(v)).lower().strip() for v in x2.iloc[i].values]
#                 if ("codigo municipio completo" in row_vals) or ("codigo_municipio_completo" in row_vals):
#                     header_row = i; break
#             if header_row is not None:
#                 x = pd.read_excel(xls_path, header=header_row, engine=engine)
#                 x = _normalize_cols(x)
#                 code_col = next((c for c in code_candidates if c in x.columns), None)
#                 name_col = next((c for c in name_candidates if c in x.columns), None)
#         except Exception:
#             pass
# 
#     if not code_col or not name_col:
#         print(f"[WARN] IBGE: nÃ£o encontrei colunas de cÃ³digo/nome. Colunas: {list(x.columns)[:10]}...")
#         return df
# 
#     x = x[[code_col, name_col]].dropna()
#     x.rename(columns={code_col:"codigo_municipio_completo", name_col:"nome_municipio_ibge"}, inplace=True)
#     x["codigo_municipio_completo"] = (
#         x["codigo_municipio_completo"].astype(str).str.replace(r"\.0$","",regex=True).str.zfill(7)
#     )
# 
#     before = len(df)
#     df_merged = df.merge(x, left_on="cod_mun", right_on="codigo_municipio_completo", how="left")
#     df_merged["nome_municipio"] = df_merged.get("nome_municipio", df_merged["cod_mun"])
#     df_merged["nome_municipio"] = df_merged["nome_municipio_ibge"].fillna(df_merged["nome_municipio"])
#     matched = df_merged["nome_municipio_ibge"].notna().sum()
#     print(f"[INFO] IBGE merge: {matched}/{before} cÃ³digos casados ({matched/before:.1%}).")
# 
#     return df_merged.drop(columns=["codigo_municipio_completo","nome_municipio_ibge"], errors="ignore")
# 
# # ==================== PrÃ©-processamento comum ====================
# def build_numeric_preprocessor(feat_cols=None):
#     # inclui features relativas por municÃ­pio-ano (rank e zscore)
#     feat_cols = feat_cols or ["ano_pib","valor_log","percentual","rank_setor_no_mun","zscore_no_mun"]
#     preproc = ColumnTransformer([
#         ("num", Pipeline([
#             ("imp", SimpleImputer(strategy="median")),
#             ("scaler", StandardScaler())
#         ]), feat_cols)
#     ], remainder="drop")
#     return preproc, feat_cols
# 
# def prepare_long_for_classification(long_df):
#     dfc = long_df.copy()
#     dfc["valor"] = pd.to_numeric(dfc["valor"], errors="coerce").clip(lower=0)
#     dfc["percentual"] = pd.to_numeric(dfc["percentual"], errors="coerce")
#     dfc["valor_log"] = np.log1p(dfc["valor"])
# 
#     # Sinais relativos por municÃ­pio-ano (100% numÃ©ricos)
#     grp = dfc.groupby(["ano_pib","cod_mun"], group_keys=False)
#     dfc["rank_setor_no_mun"] = grp["valor"].rank(ascending=False, method="dense")
#     dfc["zscore_no_mun"] = grp["valor"].transform(lambda s: (s - s.mean()) / (s.std(ddof=0) + 1e-9))
# 
#     dfc = dfc.replace([np.inf, -np.inf], np.nan)
# 
#     X = dfc[["ano_pib","valor_log","percentual","rank_setor_no_mun","zscore_no_mun"]]
#     y = dfc["setor"].astype(str)
#     return X, y, dfc
#

"""**BAYSIANO**"""

# === Debug / Preview helpers ===
def preview_df(df: pd.DataFrame, name: str, out_dir: str, n_head: int = 10, n_sample: int = 1000):
    print(f"\n===== PREVIEW: {name} =====")
    print(f"shape: {df.shape}")
    print("colunas:", list(df.columns))
    print("\n# dtypes")
    print(df.dtypes.to_string())

    print("\n# nulos por coluna (top 30):")
    nulls = df.isna().sum().sort_values(ascending=False)
    print(nulls.head(30).to_string())

    # head no console
    with pd.option_context("display.max_columns", 200, "display.width", 200):
        print(f"\n# head({n_head})")
        print(df.head(n_head))

    # dumps leves p/ disco
    os.makedirs(out_dir, exist_ok=True)
    df.head(200).to_csv(os.path.join(out_dir, f"DEBUG_{name}_head.csv"), index=False, encoding="utf-8")
    if len(df) > n_sample:
        df.sample(n_sample, random_state=42).to_csv(os.path.join(out_dir, f"DEBUG_{name}_sample_{n_sample}.csv"),
                                                    index=False, encoding="utf-8")
    else:
        df.to_csv(os.path.join(out_dir, f"DEBUG_{name}_full.csv"), index=False, encoding="utf-8")

def quick_describe_numeric(df: pd.DataFrame, name: str, out_dir: str):
    num = df.select_dtypes(include=[np.number])
    if num.empty:
        print(f"[INFO] {name}: sem colunas numÃ©ricas para describe().")
        return
    desc = num.describe(percentiles=[.01,.05,.25,.5,.75,.95,.99]).T
    print(f"\n# describe numÃ©rico â€” {name}")
    print(desc.head(20).to_string())
    desc.to_csv(os.path.join(out_dir, f"DEBUG_{name}_describe_numeric.csv"), encoding="utf-8")

# Cell 1 â€” imports & setup
import os, warnings, unicodedata
warnings.filterwarnings("ignore")

import numpy as np
np.set_printoptions(suppress=True, linewidth=160)
import pandas as pd

from sqlalchemy import create_engine, text

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report

SEED = 42
rng = np.random.default_rng(SEED)

# HiperparÃ¢metros (ajustÃ¡veis)
TABLE   = "public.pib_municipios"     # <-- AJUSTE se precisar
CUTOFF  = 2019                        # <-- ano de corte treino/teste (ajustÃ¡vel)
OUT_DIR = "/content/drive/MyDrive/PIB_Forecast"

ADVI_STEPS   = 20000                  # <-- passos de ADVI (ajustÃ¡vel)
DRAWS        = 1000                   # <-- amostras da aproximaÃ§Ã£o (ajustÃ¡vel)
LEARNING_RATE= 3e-4                   # <-- LR mais estÃ¡vel pro ADAM (ajustÃ¡vel)
PRIOR_SD     = 1.0                    # <-- sigma dos coeficientes (ajustÃ¡vel)
INTERCEPT_SD = 1.0                    # <-- sigma do intercepto (ajustÃ¡vel)
CI           = 0.90

# Cell 2 â€” utils bÃ¡sicos
def ensure_dir(p):
    os.makedirs(p, exist_ok=True)
    return p

def get_engine_from_env():
    host = os.getenv("IESB_HOST","bigdata.dataiesb.com")
    port = int(os.getenv("IESB_PORT","5432"))
    db   = os.getenv("IESB_DB","iesb")
    usr  = os.getenv("IESB_USER","data_iesb")
    pwd  = os.getenv("IESB_PWD","iesb")
    sch  = os.getenv("IESB_SCHEMA","public")
    url  = f"postgresql+psycopg2://{usr}:{pwd}@{host}:{port}/{db}"
    return create_engine(url, connect_args={"options": f"-csearch_path={sch}"})

def load_raw_table(eng, table):
    df = pd.read_sql(text(f"SELECT * FROM {table}"), con=eng)
    df.columns = [c.strip().lower() for c in df.columns]
    return df

def preview_df(df, name, n=10):
    print(f"\n===== PREVIEW: {name} =====")
    print(f"shape: {df.shape}")
    print(f"colunas: {list(df.columns)}\n")
    print("# dtypes"); print(df.dtypes.head(50)); print()
    print("# nulos por coluna (top 30):"); print(df.isna().sum().sort_values().head(30)); print()
    print(f"# head({n})"); print(df.head(n))

# Cell 2 â€” utils bÃ¡sicos
def ensure_dir(p):
    os.makedirs(p, exist_ok=True)
    return p

def get_engine_from_env():
    host = os.getenv("IESB_HOST","bigdata.dataiesb.com")
    port = int(os.getenv("IESB_PORT","5432"))
    db   = os.getenv("IESB_DB","iesb")
    usr  = os.getenv("IESB_USER","data_iesb")
    pwd  = os.getenv("IESB_PWD","iesb")
    sch  = os.getenv("IESB_SCHEMA","public")
    url  = f"postgresql+psycopg2://{usr}:{pwd}@{host}:{port}/{db}"
    return create_engine(url, connect_args={"options": f"-csearch_path={sch}"})

def load_raw_table(eng, table):
    df = pd.read_sql(text(f"SELECT * FROM {table}"), con=eng)
    df.columns = [c.strip().lower() for c in df.columns]
    return df

def preview_df(df, name, n=10):
    print(f"\n===== PREVIEW: {name} =====")
    print(f"shape: {df.shape}")
    print(f"colunas: {list(df.columns)}\n")
    print("# dtypes"); print(df.dtypes.head(50)); print()
    print("# nulos por coluna (top 30):"); print(df.isna().sum().sort_values().head(30)); print()
    print(f"# head({n})"); print(df.head(n))

# Cell 3 â€” normalizaÃ§Ã£o & melt
ALIASES = {
    "ano_pib": ["ano_pib","no_pib","ano","anopib","ano_referencia","ano_ref"],
    "cod_mun": ["cod_mun","codigo_municipio_dv","codigo_municipio","cd_municipio","co_municipio",
                "co_mun","id_municipio","cod_ibge","codigo_ibge","codigo_mun","cod_municipio"],
    "vl_agropecuaria": ["vl_agropecuaria","agropecuaria","valor_agropecuaria"],
    "vl_industria":    ["vl_industria","industria","valor_industria"],
    "vl_servicos":     ["vl_servicos","servicos","valor_servicos"],
    "vl_administracao":["vl_administracao","administracao","adm_publica","valor_administracao"],
    "vl_subsidios":    ["vl_subsidios","subsidios","valor_subsidios"],
}
SETORES_CANON = ["vl_agropecuaria","vl_industria","vl_servicos","vl_administracao","vl_subsidios"]

def _choose(cols, cands):
    for c in cands:
        if c in cols: return c
    return None

def normalize_keys(df):
    cols = df.columns
    c_ano = _choose(cols, ALIASES["ano_pib"]); c_cod = _choose(cols, ALIASES["cod_mun"])
    if not c_ano or not c_cod:
        raise ValueError(f"faltou ano/cod_mun. Colunas: {list(cols)}")
    out = df[[c_ano,c_cod]].copy().rename(columns={c_ano:"ano_pib", c_cod:"cod_mun"})
    out["ano_pib"] = pd.to_numeric(out["ano_pib"], errors="coerce").astype("Int64")
    out = out.dropna(subset=["ano_pib","cod_mun"]).copy()
    out["ano_pib"] = out["ano_pib"].astype(int)
    out["cod_mun"] = out["cod_mun"].astype(str).str.replace(r"\.0$","",regex=True).str.zfill(7)
    out["nome_municipio"] = out["cod_mun"]  # (sem merge IBGE pra evitar depender de xlrd/openpyxl)
    keep = [c for c in df.columns if c not in out.columns]
    return out.merge(df[keep], left_index=True, right_index=True, how="left")

def find_sector_columns(df):
    found = {}
    for s in SETORES_CANON:
        c = _choose(df.columns, ALIASES[s])
        if c: found[s] = c
    return found

def melt_setores(df):
    mapping = find_sector_columns(df)
    if not mapping: raise ValueError("sem colunas de setores mapeadas")
    value_cols = list(mapping.values())
    id_vars = [c for c in ["ano_pib","cod_mun","nome_municipio"] if c in df.columns]
    long = df[id_vars + value_cols].melt(id_vars=id_vars, value_vars=value_cols,
                                         var_name="setor", value_name="valor")
    inv = {v:k for k,v in mapping.items()}
    long["setor"] = long["setor"].map(inv).fillna(long["setor"])
    tot = (long.groupby(["ano_pib","cod_mun"], as_index=False)["valor"]
           .sum().rename(columns={"valor":"total_setores"}))
    long = long.merge(tot, on=["ano_pib","cod_mun"], how="left")
    long["percentual"] = np.where(long["total_setores"]>0, 100.0*long["valor"]/long["total_setores"], np.nan)
    return long

# Cell 4 â€” features & split
def build_numeric_preprocessor():
    feat_cols = ["ano_pib","valor_log","percentual"]
    pre = ColumnTransformer([
        ("num", Pipeline([
            ("imp", SimpleImputer(strategy="median")),
            ("scaler", StandardScaler()),
        ]), feat_cols)
    ], remainder="drop")
    return pre, feat_cols

def prepare_long_for_classification(long_df):
    dfc = long_df.copy()
    # garantir positividade e estabilidade numÃ©rica
    dfc["valor"] = pd.to_numeric(dfc["valor"], errors="coerce").clip(lower=0)
    dfc["percentual"] = pd.to_numeric(dfc["percentual"], errors="coerce").clip(lower=0, upper=100)
    dfc["valor_log"] = np.log1p(dfc["valor"])
    dfc = dfc.replace([np.inf,-np.inf], np.nan)
    # drop linhas sem features
    dfc = dfc.dropna(subset=["ano_pib","valor_log","percentual"])
    X = dfc[["ano_pib","valor_log","percentual"]]
    y = dfc["setor"].astype(str)
    return X, y, dfc

def temporal_split(long_df, cutoff_year=None):
    anos = np.sort(long_df["ano_pib"].unique())
    if cutoff_year is None:
        if len(anos)<2: raise ValueError("Poucos anos")
        cutoff = int(anos[-2])
    else:
        cutoff = int(cutoff_year)
    train = long_df[long_df["ano_pib"] <= cutoff].copy()
    test  = long_df[long_df["ano_pib"] >  cutoff].copy()
    if train.empty or test.empty:
        raise ValueError("Split vazio; ajuste o CUTOFF")
    return train, test, cutoff

def melt_setores(df):
    # 1) mapear colunas de setor
    mapping = find_sector_columns(df)
    if not mapping:
        raise ValueError("sem colunas de setores mapeadas")
    value_cols = list(mapping.values())

    # 2) manter id_vars
    id_vars = [c for c in ["ano_pib","cod_mun","nome_municipio"] if c in df.columns]

    # 3) sÃ³ garantir dtype float e zerar negativos (use where p/ manter NaN onde nÃ£o hÃ¡ dado)
    vals = df[value_cols].apply(pd.to_numeric, errors="coerce").astype("float64")
    vals_pos = vals.where(vals >= 0.0, other=0.0)   # <â€” negativos viram 0

    # 4) total apenas do que Ã© positivo
    total_pos = vals_pos.sum(axis=1).astype("float64")

    # 5) reconstruir dataframe com ids + valores ajustados
    df_fixed = pd.concat([df[id_vars].reset_index(drop=True),
                          vals_pos.reset_index(drop=True)], axis=1)

    # 6) melt
    long = df_fixed.melt(id_vars=id_vars, value_vars=value_cols,
                         var_name="setor", value_name="valor")
    inv = {v: k for k, v in mapping.items()}
    long["setor"] = long["setor"].map(inv).fillna(long["setor"])

    # 7) anexar total positivo correto (repetindo por linha original)
    # total_pos estÃ¡ na escala do df original; precisamos repetir conforme o melt
    reps = len(value_cols)
    long["total_setores"] = np.repeat(total_pos.values, repeats=reps).astype("float64")

    # 8) percentual seguro
    long["percentual"] = np.where(long["total_setores"] > 0.0,
                                  100.0 * long["valor"] / long["total_setores"],
                                  np.nan)
    # 9) clamp para [0, 100]
    long["percentual"] = long["percentual"].clip(lower=0.0, upper=100.0)

    return long

# Cell 5 â€” carga e melt
out_dir = ensure_dir(OUT_DIR)
eng = get_engine_from_env()

print("[INFO] Carregando tabela...")
df_raw = load_raw_table(eng, TABLE)
preview_df(df_raw, "raw")

print("\n[INFO] Normalizando chaves...")
df_norm = normalize_keys(df_raw)

print("[INFO] Melt & percentuais...")
df_long = melt_setores(df_norm)
preview_df(df_long, "long")

# Cell 6 â€” split & preprocess
train_long, test_long, cutoff = temporal_split(df_long, cutoff_year=CUTOFF)

Xtr, ytr, _   = prepare_long_for_classification(train_long)
Xte, yte, dte = prepare_long_for_classification(test_long)

pre, feat_cols = build_numeric_preprocessor()
pre = pre.fit(Xtr)

# Checagens rÃ¡pidas
print(f"cutoff = {cutoff}")
print("Xtr/Xte shapes:", Xtr.shape, Xte.shape)
print("classes treino:", sorted(pd.Series(ytr).unique()))
print(pd.Series(ytr).value_counts())

# Cell 7 â€” PyMC: softmax VI (ADVI) apenas
import pymc as pm
try:
    import pytensor.tensor as pt
except Exception:
    import aesara.tensor as pt

def fit_multiclass_advi(pre, Xtr, ytr_str, Xte, ci=0.90, draws=1000, steps=20000, lr=3e-4,
                        prior_sd=1.0, intercept_sd=1.0, verbose=False):
    # transforma com o MESMO pre
    Xtr_p = pre.transform(Xtr)
    Xte_p = pre.transform(Xte)

    # limpa NaNs pÃ³s-transform
    mask_tr = ~np.isnan(Xtr_p).any(axis=1)
    mask_te = ~np.isnan(Xte_p).any(axis=1)
    if not mask_tr.all():
        Xtr_p = Xtr_p[mask_tr]; ytr_str = pd.Series(ytr_str).iloc[mask_tr].astype(str).values
    if not mask_te.all():
        Xte_p = Xte_p[mask_te]

    classes = np.array(sorted(pd.Series(ytr_str).astype(str).unique()))
    K = len(classes)
    y_idx = pd.Series(ytr_str).map({c:i for i,c in enumerate(classes)}).astype("int64").values
    nfeat = Xtr_p.shape[1]

    alpha = (1.0 - ci)/2.0
    plo, phi = 100*alpha, 100*(1-alpha)

    if K < 2:
        raise ValueError("Precisa de ao menos 2 classes no treino.")
    if K == 2:
        # (Se K==2, modelamos como softmax com K-1=1 da mesma forma)
        pass

    with pm.Model() as m:
        B  = pm.Normal("B", 0, prior_sd,     shape=(nfeat, K-1))
        b0 = pm.Normal("b0",0, intercept_sd, shape=(K-1,))
        eta_km1 = pt.dot(Xtr_p, B) + b0                 # [N, K-1]
        eta     = pt.concatenate([eta_km1, pt.zeros((Xtr_p.shape[0],1))], axis=1)
        p_tr    = pm.math.softmax(eta)
        pm.Categorical("y_obs", p=p_tr, observed=y_idx)

        approx = pm.fit(
            n=steps, method="advi",
            obj_optimizer=pm.adam(learning_rate=lr),
            callbacks=[pm.callbacks.CheckParametersConvergence(tolerance=1e-2, diff='absolute')],
            progressbar=verbose
        )
        idata = approx.sample(draws=draws)

    # prediÃ§Ã£o em Xte
    B_s  = np.asarray(idata.posterior["B"]).reshape(-1, nfeat, K-1)
    b0_s = np.asarray(idata.posterior["b0"]).reshape(-1, K-1)
    S    = B_s.shape[0]

    logits_km1 = np.einsum("sfk,nf->snk", B_s, Xte_p) + b0_s[:,None,:]   # [S,N,K-1]
    logits = np.concatenate([logits_km1, np.zeros((S, Xte_p.shape[0],1))], axis=2)  # [S,N,K]
    ex     = np.exp(logits - logits.max(axis=2, keepdims=True))
    probs  = ex / ex.sum(axis=2, keepdims=True)                             # [S,N,K]

    proba_mean = probs.mean(axis=0)
    proba_lo   = np.percentile(probs, plo, axis=0)
    proba_hi   = np.percentile(probs, phi, axis=0)
    return classes, proba_mean, proba_lo, proba_hi

# Cell 8 â€” treino & avaliaÃ§Ã£o
print("[INFO] Treinando BAYES (ADVI, sem NUTS)...")
classes_b, p_mean_b, p_lo_b, p_hi_b = fit_multiclass_advi(
    pre, Xtr, ytr, Xte,
    ci=CI, draws=DRAWS, steps=ADVI_STEPS, lr=LEARNING_RATE,
    prior_sd=PRIOR_SD, intercept_sd=INTERCEPT_SD, verbose=False
)

ypred_idx = p_mean_b.argmax(axis=1)
ypred = [classes_b[i] for i in ypred_idx]

acc  = accuracy_score(yte, ypred)
f1m  = f1_score(yte, ypred, average="macro")
cm   = confusion_matrix(yte, ypred, labels=classes_b)

print(f"[OK] BAYES (ADVI) -> acc={acc:.3f} | f1_macro={f1m:.3f}")
print("\n=== Classification report (Bayes/ADVI) ===")
print(classification_report(yte, ypred, labels=classes_b, zero_division=0))
print("\n=== Confusion matrix (labels em ordem de classes_b) ===")
print(classes_b)
print(cm)

# Cell â€” salvar resultados do BAYES/ADVI no Drive (para o Streamlit)
import os
import numpy as np
import pandas as pd
from sklearn.metrics import classification_report

OUT_DIR = "/content/drive/MyDrive/PIB_Forecast"
os.makedirs(OUT_DIR, exist_ok=True)

# 1) Tabela de resultados por linha (test set)
res = dte[["ano_pib","cod_mun","nome_municipio","setor","valor","percentual"]].copy()
for j, c in enumerate(classes_b):
    res[f"p_bayes_mean__{c}"] = p_mean_b[:, j]
    res[f"p_bayes_lo__{c}"]   = p_lo_b[:, j]
    res[f"p_bayes_hi__{c}"]   = p_hi_b[:, j]

best_idx = p_mean_b.argmax(axis=1)
res["pred_setor_bayes"]   = [classes_b[i] for i in best_idx]
res["proba_max_bayes"]    = p_mean_b.max(axis=1)
res["proba_max_bayes_lo"] = p_lo_b[np.arange(len(best_idx)), best_idx]
res["proba_max_bayes_hi"] = p_hi_b[np.arange(len(best_idx)), best_idx]

res_path = os.path.join(OUT_DIR, "results_classify_test_BAYES.csv")
res.to_csv(res_path, index=False, encoding="utf-8")

# 2) MÃ©tricas resumo
summary = pd.DataFrame([{
    "cutoff": int(dte["ano_pib"].min()) - 1 if "ano_pib" in dte.columns else None,
    "accuracy_bayes": float(acc),
    "f1_macro_bayes": float(f1m)
}])
summary_path = os.path.join(OUT_DIR, "metrics_classify_summary_BAYES.csv")
summary.to_csv(summary_path, index=False, encoding="utf-8")

# 3) Matriz de confusÃ£o
cm_df = pd.DataFrame(cm, index=classes_b, columns=classes_b)
cm_path = os.path.join(OUT_DIR, "metrics_classify_confusion_matrix_BAYES.csv")
cm_df.to_csv(cm_path, encoding="utf-8")

# 4) Classification report (opcional) em .txt
report_txt = classification_report(yte, [classes_b[i] for i in best_idx], labels=classes_b, zero_division=0)
with open(os.path.join(OUT_DIR, "classification_report_BAYES.txt"), "w", encoding="utf-8") as f:
    f.write(report_txt)

print("Arquivos salvos:")
print(res_path)
print(summary_path)
print(cm_path)
print(os.path.join(OUT_DIR, "classification_report_BAYES.txt"))

# Cell â€” treino & avaliaÃ§Ã£o + exportaÃ§Ã£o para CSV
print("[INFO] Treinando BAYES (ADVI, sem NUTS)...")
classes_b, p_mean_b, p_lo_b, p_hi_b = fit_multiclass_advi(
    pre, Xtr, ytr, Xte,
    ci=CI, draws=DRAWS, steps=ADVI_STEPS, lr=LEARNING_RATE,
    prior_sd=PRIOR_SD, intercept_sd=INTERCEPT_SD, verbose=False
)

ypred_idx = p_mean_b.argmax(axis=1)
ypred = [classes_b[i] for i in ypred_idx]

acc  = accuracy_score(yte, ypred)
f1m  = f1_score(yte, ypred, average="macro")
cm   = confusion_matrix(yte, ypred, labels=classes_b)

print(f"[OK] BAYES (ADVI) -> acc={acc:.3f} | f1_macro={f1m:.3f}")
print("\n=== Classification report (Bayes/ADVI) ===")
print(classification_report(yte, ypred, labels=classes_b, zero_division=0))
print("\n=== Confusion matrix (labels em ordem de classes_b) ===")
print(classes_b)
print(cm)

# ========== EXPORTAÃ‡ÃƒO PARA PASTA ==========
import os
OUT_DIR = "/content/drive/MyDrive/PIB_Forecast"
os.makedirs(OUT_DIR, exist_ok=True)

# resultados detalhados
res = dte[["ano_pib","cod_mun","nome_municipio","setor","valor","percentual"]].copy()
for j,c in enumerate(classes_b):
    res[f"p_bayes_mean__{c}"] = p_mean_b[:,j]
    res[f"p_bayes_lo__{c}"]   = p_lo_b[:,j]
    res[f"p_bayes_hi__{c}"]   = p_hi_b[:,j]

res["pred_setor_bayes"]   = ypred
res["proba_max_bayes"]    = p_mean_b.max(axis=1)
res["proba_max_bayes_lo"] = p_lo_b[np.arange(len(ypred_idx)), ypred_idx]
res["proba_max_bayes_hi"] = p_hi_b[np.arange(len(ypred_idx)), ypred_idx]

# salva arquivos
res.to_csv(os.path.join(OUT_DIR, "results_classify_test_BAYES.csv"), index=False, encoding="utf-8")
pd.DataFrame([{"accuracy": acc, "f1_macro": f1m}]).to_csv(
    os.path.join(OUT_DIR, "metrics_classify_summary_BAYES.csv"),
    index=False, encoding="utf-8"
)
pd.DataFrame(cm, index=classes_b, columns=classes_b).to_csv(
    os.path.join(OUT_DIR, "metrics_classify_confusion_matrix_BAYES.csv"),
    encoding="utf-8"
)

print(f"[OK] Resultados exportados

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/PIB_Forecast/train_classify_setor_bayes.py
# # -*- coding: utf-8 -*-
# import os, warnings, argparse, unicodedata
# warnings.filterwarnings("ignore")
# 
# import numpy as np
# import pandas as pd
# from sqlalchemy import create_engine, text
# from sklearn.pipeline import Pipeline
# from sklearn.compose import ColumnTransformer
# from sklearn.preprocessing import StandardScaler
# from sklearn.impute import SimpleImputer
# from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
# 
# # ========== Utils bÃ¡sicos ==========
# def ensure_dir(p): os.makedirs(p, exist_ok=True); return p
# 
# def get_engine_from_env(args):
#     host = args.db_host or os.getenv("IESB_HOST","bigdata.dataiesb.com")
#     port = int(args.db_port or os.getenv("IESB_PORT","5432"))
#     db   = args.db_name or os.getenv("IESB_DB","iesb")
#     usr  = args.db_user or os.getenv("IESB_USER","data_iesb")
#     pwd  = args.db_pwd  or os.getenv("IESB_PWD","iesb")
#     sch  = args.db_schema or os.getenv("IESB_SCHEMA","public")
#     url  = f"postgresql+psycopg2://{usr}:{pwd}@{host}:{port}/{db}"
#     return create_engine(url, connect_args={"options": f"-csearch_path={sch}"})
# 
# def load_raw_table(eng, table):
#     df = pd.read_sql(text(f"SELECT * FROM {table}"), con=eng)
#     df.columns = [c.strip().lower() for c in df.columns]
#     return df
# 
# # ========== NormalizaÃ§Ã£o / IBGE ==========
# ALIASES = {
#     "ano_pib": ["ano_pib","no_pib","ano","anopib","ano_referencia","ano_ref"],
#     "cod_mun": ["cod_mun","codigo_municipio_dv","codigo_municipio","cd_municipio","co_municipio",
#                 "co_mun","id_municipio","cod_ibge","codigo_ibge","codigo_mun","cod_municipio"],
#     "vl_agropecuaria": ["vl_agropecuaria","agropecuaria","valor_agropecuaria"],
#     "vl_industria":    ["vl_industria","industria","valor_industria"],
#     "vl_servicos":     ["vl_servicos","servicos","valor_servicos"],
#     "vl_administracao":["vl_administracao","administracao","adm_publica","valor_administracao"],
#     "vl_subsidios":    ["vl_subsidios","subsidios","valor_subsidios"],
# }
# SETORES_CANON = ["vl_agropecuaria","vl_industria","vl_servicos","vl_administracao","vl_subsidios"]
# 
# def _choose(cols, cands):
#     for c in cands:
#         if c in cols: return c
#     return None
# 
# def normalize_keys(df):
#     cols = df.columns
#     c_ano = _choose(cols, ALIASES["ano_pib"]); c_cod = _choose(cols, ALIASES["cod_mun"])
#     if not c_ano or not c_cod:
#         raise ValueError(f"faltou ano/cod_mun. Colunas: {list(cols)}")
#     out = df[[c_ano,c_cod]].copy().rename(columns={c_ano:"ano_pib", c_cod:"cod_mun"})
#     out["ano_pib"] = pd.to_numeric(out["ano_pib"], errors="coerce").astype("Int64")
#     out = out.dropna(subset=["ano_pib","cod_mun"]).copy()
#     out["ano_pib"] = out["ano_pib"].astype(int)
#     out["cod_mun"] = out["cod_mun"].astype(str).str.replace(r"\.0$","",regex=True).str.zfill(7)
#     out["nome_municipio"] = out["cod_mun"]
#     keep = [c for c in df.columns if c not in out.columns]
#     return out.merge(df[keep], left_index=True, right_index=True, how="left")
# 
# def _strip_accents(s):
#     if not isinstance(s, str): s = str(s)
#     return ''.join(ch for ch in unicodedata.normalize('NFKD', s) if not unicodedata.combining(ch))
# 
# def _normalize_cols(df):
#     cols = []
#     for c in df.columns:
#         base = _strip_accents(str(c)).lower().strip().replace(" ","_")
#         base = "".join(ch if ch.isalnum() or ch=="_" else "_" for ch in base)
#         cols.append(base)
#     df2 = df.copy(); df2.columns = cols; return df2
# 
# def try_merge_ibge(df, xls_path):
#     if not xls_path or not os.path.exists(xls_path):
#         print("[INFO] sem IBGE XLS/XLSX â€” pulando"); return df
#     ext = os.path.splitext(xls_path)[1].lower()
#     engine = "openpyxl" if ext==".xlsx" else "xlrd"
#     try:
#         x = pd.read_excel(xls_path, header=6, engine=engine)
#     except Exception:
#         x = pd.read_excel(xls_path, header=0, engine=engine)
#     x = _normalize_cols(x)
#     code_candidates = ["codigo_municipio_completo","codigo_municipio","cod_municipio",
#                        "codigo_municipio_ibge","codigo_ibge","codigo_mun","cod_ibge"]
#     name_candidates = ["nome_municipio","nome_municipio_","nome_municipio_ibge","nome_do_municipio","nome"]
#     code_col = next((c for c in code_candidates if c in x.columns), None)
#     name_col = next((c for c in name_candidates if c in x.columns), None)
#     if not code_col or not name_col:
#         print(f"[WARN] IBGE: nÃ£o encontrei colunas. Colunas={list(x.columns)[:8]}"); return df
#     x = x[[code_col,name_col]].dropna().rename(columns={code_col:"codigo_municipio_completo", name_col:"nome_municipio_ibge"})
#     x["codigo_municipio_completo"] = x["codigo_municipio_completo"].astype(str).str.replace(r"\.0$","",regex=True).str.zfill(7)
#     dfm = df.merge(x, left_on="cod_mun", right_on="codigo_municipio_completo", how="left")
#     dfm["nome_municipio"] = dfm.get("nome_municipio", dfm["cod_mun"])
#     dfm["nome_municipio"] = dfm["nome_municipio_ibge"].fillna(dfm["nome_municipio"])
#     return dfm.drop(columns=["codigo_municipio_completo","nome_municipio_ibge"], errors="ignore")
# 
# def find_sector_columns(df):
#     found = {}
#     for s in SETORES_CANON:
#         c = _choose(df.columns, ALIASES[s])
#         if c: found[s] = c
#     return found
# 
# # *** NOVO: robusto a negativos e percentuais absurdos
# def melt_setores(df):
#     mapping = find_sector_columns(df)
#     if not mapping: raise ValueError("sem colunas de setores mapeadas")
#     value_cols = list(mapping.values())
#     id_vars = [c for c in ["ano_pib","cod_mun","nome_municipio"] if c in df.columns]
# 
#     # valores numÃ©ricos e zera negativos
#     vals = df[value_cols].apply(pd.to_numeric, errors="coerce").astype("float64")
#     vals_pos = vals.where(vals >= 0.0, other=0.0)
# 
#     # total apenas do positivo
#     total_pos = vals_pos.sum(axis=1).astype("float64")
# 
#     # reconstrÃ³i df fixo e derrete
#     df_fixed = pd.concat([df[id_vars].reset_index(drop=True), vals_pos.reset_index(drop=True)], axis=1)
#     long = df_fixed.melt(id_vars=id_vars, value_vars=value_cols, var_name="setor", value_name="valor")
#     inv = {v:k for k,v in mapping.items()}
#     long["setor"] = long["setor"].map(inv).fillna(long["setor"])
# 
#     # injeta total e calcula percentual seguro
#     reps = len(value_cols)
#     long["total_setores"] = np.repeat(total_pos.values, repeats=reps).astype("float64")
#     long["percentual"] = np.where(long["total_setores"] > 0.0, 100.0*long["valor"]/long["total_setores"], np.nan)
#     long["percentual"] = long["percentual"].clip(lower=0.0, upper=100.0)
# 
#     return long
# 
# # ========== PrÃ©-processador ==========
# def build_numeric_preprocessor():
#     feat_cols = ["ano_pib","valor_log","percentual"]
#     pre = ColumnTransformer([
#         ("num", Pipeline([
#             ("imp", SimpleImputer(strategy="median")),
#             ("scaler", StandardScaler()),
#         ]), feat_cols)
#     ], remainder="drop")
#     return pre, feat_cols
# 
# def prepare_long_for_classification(long_df):
#     dfc = long_df.copy()
#     dfc["valor"] = pd.to_numeric(dfc["valor"], errors="coerce").clip(lower=0)  # jÃ¡ vem >=0 do melt robusto
#     dfc["percentual"] = pd.to_numeric(dfc["percentual"], errors="coerce").clip(lower=0, upper=100)
#     dfc["valor_log"] = np.log1p(dfc["valor"])
#     dfc = dfc.replace([np.inf,-np.inf], np.nan)
#     X = dfc[["ano_pib","valor_log","percentual"]]
#     y = dfc["setor"].astype(str)
#     return X, y, dfc
# 
# # ========== BAYES (sÃ³ VI/ADVI â€” SEM NUTS) ==========
# def fit_bayes_vi(preprocessor, Xtr, ytr_str, Xte, ci=0.90, draws=1000, advi_steps=20000, lr=5e-4):
#     """
#     - K==2: logÃ­stica binÃ¡ria com VI.
#     - K>=3: softmax (multiclasse) com ADVI.
#     SEM NUTS.
#     """
#     try:
#         import pymc as pm
#         try:
#             import pytensor.tensor as pt
#         except Exception:
#             import aesara.tensor as pt
#         import arviz as az  # sÃ³ para checagens internas (nÃ£o obrigatÃ³rio)
#     except Exception as e:
#         return False, f"PyMC indisponÃ­vel: {e}", None, None, None, None
# 
#     Xtr_p = preprocessor.transform(Xtr)
#     Xte_p = preprocessor.transform(Xte)
# 
#     # sanity antes de treinar
#     if np.isnan(Xtr_p).any() or np.isinf(Xtr_p).any():
#         return False, "Xtr_p contÃ©m NaN/Inf apÃ³s preprocess.", None, None, None, None
#     if np.isnan(Xte_p).any() or np.isinf(Xte_p).any():
#         return False, "Xte_p contÃ©m NaN/Inf apÃ³s preprocess.", None, None, None, None
# 
#     classes = np.array(sorted(pd.Series(ytr_str).astype(str).unique()))
#     K = len(classes)
#     alpha = (1.0 - ci)/2.0
#     plo, phi = 100*alpha, 100*(1-alpha)
#     nfeat = Xtr_p.shape[1]
# 
#     if K == 2:
#         # ---- BinÃ¡rio via VI ----
#         y_bin = (pd.Series(ytr_str).astype(str).values == classes[1]).astype("int8")
#         with pm.Model() as m:
#             coefs = pm.Normal("coefs", 0, 1.0, shape=nfeat)
#             intercept = pm.Normal("intercept", 0, 1.0)
#             logits = intercept + pt.dot(Xtr_p, coefs)
#             p = pm.math.sigmoid(logits)
#             pm.Bernoulli("y_obs", p=p, observed=y_bin)
#             approx = pm.fit(
#                 n=advi_steps, method="advi",
#                 obj_optimizer=pm.adam(learning_rate=lr),
#                 callbacks=[pm.callbacks.CheckParametersConvergence(tolerance=1e-2, diff="absolute")],
#                 progressbar=False
#             )
#             idata = approx.sample(draws=draws)
# 
#         coefs_s = np.asarray(idata.posterior["coefs"]).reshape(-1, nfeat)
#         inter_s = np.asarray(idata.posterior["intercept"]).reshape(-1)
#         logits_te = (Xte_p @ coefs_s.T) + inter_s
#         pte = 1/(1+np.exp(-logits_te))
#         p_mean = pte.mean(axis=1); p_lo = np.percentile(pte, plo, axis=1); p_hi = np.percentile(pte, phi, axis=1)
#         proba_mean = np.vstack([1-p_mean, p_mean]).T
#         proba_lo   = np.vstack([1-p_hi,   p_lo  ]).T
#         proba_hi   = np.vstack([1-p_lo,   p_hi  ]).T
#         return True, None, classes, proba_mean, proba_lo, proba_hi
# 
#     # ---- Multiclasse (Softmax) via ADVI ----
#     y_idx = pd.Series(ytr_str).astype(str).map({c:i for i,c in enumerate(classes)}).values.astype("int64")
#     with pm.Model() as m:
#         # priors conservadores ajudam a estabilidade
#         B  = pm.Normal("B", 0, 1.0, shape=(nfeat, K-1))
#         b0 = pm.Normal("b0",0, 1.0, shape=(K-1,))
#         eta_km1 = Xtr_p @ B + b0
#         eta = pm.math.concatenate([eta_km1, pm.math.zeros((Xtr_p.shape[0],1))], axis=1)  # classe de referÃªncia
#         p_tr = pm.math.softmax(eta)
#         pm.Categorical("y_obs", p=p_tr, observed=y_idx)
# 
#         approx = pm.fit(
#             n=advi_steps, method="advi",
#             obj_optimizer=pm.adam(learning_rate=lr),
#             callbacks=[pm.callbacks.CheckParametersConvergence(tolerance=1e-2, diff="absolute")],
#             progressbar=False
#         )
#         idata = approx.sample(draws=draws)
# 
#     B_s  = np.asarray(idata.posterior["B"]).reshape(-1, nfeat, K-1)
#     b0_s = np.asarray(idata.posterior["b0"]).reshape(-1, K-1)
#     S = B_s.shape[0]
#     logits_km1 = np.einsum("sfk,nf->snk", B_s, Xte_p) + b0_s[:,None,:]
#     logits = np.concatenate([logits_km1, np.zeros((S, Xte_p.shape[0],1))], axis=2)
#     ex = np.exp(logits - logits.max(axis=2, keepdims=True))
#     probs = ex / ex.sum(axis=2, keepdims=True)
# 
#     proba_mean = probs.mean(axis=0)
#     proba_lo   = np.percentile(probs, plo, axis=0)
#     proba_hi   = np.percentile(probs, phi, axis=0)
#     return True, None, classes, proba_mean, proba_lo, proba_hi
# 
# # ========== MAIN ==========
# def main():
#     p = argparse.ArgumentParser(description="ClassificaÃ§Ã£o de SETOR â€” Bayes (VI/ADVI, sem NUTS).")
#     # DB
#     p.add_argument("--db_host", default=None); p.add_argument("--db_port", default=None)
#     p.add_argument("--db_name", default=None); p.add_argument("--db_user", default=None)
#     p.add_argument("--db_pwd",  default=None); p.add_argument("--db_schema", default=None)
#     p.add_argument("--table", default="public.pib_municipios")
#     # I/O
#     p.add_argument("--out_dir", default="/content/drive/MyDrive/PIB_Forecast")
#     p.add_argument("--ibge_xls_path", default="/content/drive/MyDrive/Bases de Dados/RELATORIO_DTB_BRASIL_2024_MUNICIPIOS.xls")
#     # split
#     p.add_argument("--cutoff_year", type=int, default=None)
#     # bayes
#     p.add_argument("--bayes_draws", type=int, default=1000)
#     p.add_argument("--advi_steps", type=int, default=20000)
#     p.add_argument("--ci", type=float, default=0.90)
#     args, _ = p.parse_known_args()
# 
#     eng = get_engine_from_env(args)
#     out_dir = ensure_dir(args.out_dir)
# 
#     print("[INFO] Carregando tabela..."); df_raw = load_raw_table(eng, args.table)
#     print("[INFO] Normalizando chaves..."); df_norm = normalize_keys(df_raw)
#     df_norm = try_merge_ibge(df_norm, args.ibge_xls_path)
#     print("[INFO] Melt & percentuais..."); df_long = melt_setores(df_norm)
# 
#     # split temporal
#     anos = np.sort(df_long["ano_pib"].unique())
#     if args.cutoff_year is None:
#         if len(anos)<2: raise ValueError("Poucos anos")
#         cutoff = int(anos[-2])
#     else:
#         cutoff = int(args.cutoff_year)
# 
#     train = df_long[df_long["ano_pib"] <= cutoff].copy()
#     test  = df_long[df_long["ano_pib"] >  cutoff].copy()
#     if train.empty or test.empty: raise ValueError("Split vazio; ajuste --cutoff_year")
# 
#     Xtr, ytr, _   = prepare_long_for_classification(train)
#     Xte, yte, dte = prepare_long_for_classification(test)
# 
#     # prÃ©-processador numÃ©rico (fit no treino)
#     pre, _ = build_numeric_preprocessor()
#     pre = pre.fit(Xtr)
# 
#     # ----- Bayes (SEM NUTS) -----
#     print("[INFO] Treinando BAYES (VI/ADVI, sem NUTS)...")
#     ok, msg, classes_b, p_mean_b, p_lo_b, p_hi_b = fit_bayes_vi(
#         preprocessor=pre, Xtr=Xtr, ytr_str=ytr, Xte=Xte,
#         ci=args.ci, draws=args.bayes_draws, advi_steps=args.advi_steps, lr=5e-4
#     )
#     if not ok:
#         print(f"[ERRO] Bayes falhou: {msg}"); return
# 
#     ypred = [classes_b[i] for i in p_mean_b.argmax(axis=1)]
#     acc = accuracy_score(yte, ypred)
#     f1m = f1_score(yte, ypred, average="macro")
#     cm  = confusion_matrix(yte, ypred, labels=classes_b)
# 
#     # salvar resultados
#     res = dte[["ano_pib","cod_mun","nome_municipio","setor","valor","percentual"]].copy()
#     for j,c in enumerate(classes_b):
#         res[f"p_bayes_mean__{c}"] = p_mean_b[:,j]
#         res[f"p_bayes_lo__{c}"]   = p_lo_b[:,j]
#         res[f"p_bayes_hi__{c}"]   = p_hi_b[:,j]
#     idx = p_mean_b.argmax(axis=1)
#     res["pred_setor_bayes"] = [classes_b[i] for i in idx]
#     res["proba_max_bayes"]  = p_mean_b.max(axis=1)
#     res["proba_max_bayes_lo"] = p_lo_b[np.arange(len(idx)), idx]
#     res["proba_max_bayes_hi"] = p_hi_b[np.arange(len(idx)), idx]
# 
#     res.to_csv(os.path.join(out_dir, "results_classify_test_BAYES.csv"), index=False, encoding="utf-8")
#     pd.DataFrame([{"cutoff": cutoff, "accuracy_bayes": acc, "f1_macro_bayes": f1m}])\
#       .to_csv(os.path.join(out_dir, "metrics_classify_summary_BAYES.csv"), index=False, encoding="utf-8")
#     pd.DataFrame(cm, index=classes_b, columns=classes_b)\
#       .to_csv(os.path.join(out_dir, "metrics_classify_confusion_matrix_BAYES.csv"), encoding="utf-8")
# 
#     print(f"[OK] BAYES (sem NUTS) -> acc={acc:.3f} | f1={f1m:.3f}")
# 
# if __name__ == "__main__":
#     main()
#

!python /content/drive/MyDrive/PIB_Forecast/train_classify_setor_bayes.py \
  --table public.pib_municipios \
  --cutoff_year 2019 \
  --advi_steps 20000 \
  --bayes_draws 1000 \
  --ci 0.90 \
  --out_dir "/content/drive/MyDrive/PIB_Forecast"

"""# **STREAMLIT**"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/PIB_Forecast/train_classify_setor.py
# # -*- coding: utf-8 -*-
# import os, warnings, argparse, unicodedata
# warnings.filterwarnings("ignore")
# 
# import numpy as np
# import pandas as pd
# from sqlalchemy import create_engine, text
# 
# from sklearn.pipeline import Pipeline
# from sklearn.compose import ColumnTransformer
# from sklearn.preprocessing import StandardScaler
# from sklearn.impute import SimpleImputer
# from sklearn.linear_model import LogisticRegression
# from sklearn.metrics import accuracy_score, f1_score, confusion_matrix
# import builtins as _py_builtins
# 
# # -------------------- PadronizaÃ§Ã£o / SanitizaÃ§Ã£o --------------------
# _NUMPY_RESERVED = {
#     "sum","mean","std","var","min","max","median","average","clip","round","log","log1p","exp",
#     "sin","cos","tan","arcsin","arccos","arctan","sqrt","abs","sign","where","all","any","prod",
#     "cumsum","cumprod","argsort","argmin","argmax","astype","dtype","shape","size","ndim","ndarray",
#     "array","matrix","transpose","dot","einsum","unique","isnan","isfinite","isinf","conj","imag","real",
#     "nan","inf","pi","e","load","save","fromfile","tofile"
# }
# _BUILTIN_RESERVED = {n for n in dir(_py_builtins)}
# _RESERVED = {s.lower() for s in (_NUMPY_RESERVED | _BUILTIN_RESERVED)}
# 
# def sanitize_columns(df: pd.DataFrame) -> pd.DataFrame:
#     new_cols = []
#     for c in df.columns:
#         base = str(c).strip().lower().replace(" ", "_")
#         base = "".join(ch if ch.isalnum() or ch == "_" else "_" for ch in base)
#         if base in _RESERVED:
#             base = f"{base}_col"
#         new_cols.append(base)
#     out = df.copy()
#     out.columns = new_cols
#     return out
# 
# # -------------------- ALIASES --------------------
# ALIASES = {
#     "ano_pib": ["ano_pib","no_pib","ano","anopib","ano_referencia","ano_ref"],
#     "cod_mun": ["cod_mun","codigo_municipio_dv","codigo_municipio","cd_municipio","co_municipio",
#                 "co_mun","id_municipio","cod_ibge","codigo_ibge","codigo_mun","cod_municipio"],
#     "vl_agropecuaria": ["vl_agropecuaria","agropecuaria","valor_agropecuaria"],
#     "vl_industria":    ["vl_industria","industria","valor_industria"],
#     "vl_servicos":     ["vl_servicos","servicos","valor_servicos"],
#     "vl_administracao":["vl_administracao","administracao","adm_publica","valor_administracao"],
#     "vl_subsidios":    ["vl_subsidios","subsidios","valor_subsidios"],
# }
# SETORES_CANON = ["vl_agropecuaria","vl_industria","vl_servicos","vl_administracao","vl_subsidios"]
# 
# def _choose(cols, candidates):
#     for c in candidates:
#         if c in cols:
#             return c
#     return None
# 
# def _ensure_dir(p):
#     os.makedirs(p, exist_ok=True)
#     return p
# 
# # -------------------- DB --------------------
# def get_engine_from_env(args):
#     host = args.db_host or os.getenv("IESB_HOST","bigdata.dataiesb.com")
#     port = int(args.db_port or os.getenv("IESB_PORT","5432"))
#     db   = args.db_name or os.getenv("IESB_DB","iesb")
#     usr  = args.db_user or os.getenv("IESB_USER","data_iesb")
#     pwd  = args.db_pwd  or os.getenv("IESB_PWD","iesb")
#     sch  = args.db_schema or os.getenv("IESB_SCHEMA","public")
#     url  = f"postgresql+psycopg2://{usr}:{pwd}@{host}:{port}/{db}"
#     return create_engine(url, connect_args={"options": f"-csearch_path={sch}"})
# 
# def load_raw_table(eng, table):
#     df = pd.read_sql(text(f"SELECT * FROM {table}"), con=eng)
#     return sanitize_columns(df)
# 
# # -------------------- NormalizaÃ§Ã£o / Melt --------------------
# def normalize_keys(df):
#     cols = df.columns
#     c_ano = _choose(cols, ALIASES["ano_pib"]); c_cod = _choose(cols, ALIASES["cod_mun"])
#     if not c_ano or not c_cod:
#         raise ValueError(f"faltou ano/cod_mun. Colunas: {list(cols)}")
# 
#     df_keys = df[[c_ano, c_cod]].copy().rename(columns={c_ano:"ano_pib", c_cod:"cod_mun"})
#     df_keys["ano_pib"] = pd.to_numeric(df_keys["ano_pib"], errors="coerce").astype("Int64")
#     df_keys = df_keys.dropna(subset=["ano_pib","cod_mun"]).copy()
#     df_keys["ano_pib"] = df_keys["ano_pib"].astype(int)
#     df_keys["cod_mun"] = df_keys["cod_mun"].astype(str).str.replace(r"\.0$","",regex=True).str.zfill(7)
#     df_keys["nome_municipio"] = df_keys["cod_mun"]
# 
#     keep_cols = [c for c in df.columns if c not in df_keys.columns]
#     return df_keys.merge(df[keep_cols], left_index=True, right_index=True, how="left")
# 
# def find_sector_columns(df):
#     cols = df.columns
#     found = {}
#     for canon in SETORES_CANON:
#         c = _choose(cols, ALIASES[canon])
#         if c:
#             found[canon] = c
#     return found
# 
# def melt_setores(df):
#     mapping = find_sector_columns(df)
#     if not mapping:
#         raise ValueError("sem colunas de setores mapeadas (apÃ³s sanitize)")
#     value_cols = list(mapping.values())
#     id_vars = [c for c in ["ano_pib","cod_mun","nome_municipio"] if c in df.columns]
# 
#     df_long = df[id_vars + value_cols].melt(
#         id_vars=id_vars, value_vars=value_cols, var_name="setor", value_name="valor"
#     )
#     inv = {v:k for k,v in mapping.items()}
#     df_long["setor"] = df_long["setor"].map(inv).fillna(df_long["setor"])
# 
#     df_total = (
#         df_long.groupby(["ano_pib","cod_mun"], as_index=False)["valor"]
#         .sum()
#         .rename(columns={"valor":"total_setores"})
#     )
#     df_long = df_long.merge(df_total, on=["ano_pib","cod_mun"], how="left")
#     df_long["percentual"] = np.where(
#         df_long["total_setores"]>0, 100.0*df_long["valor"]/df_long["total_setores"], np.nan
#     )
#     return df_long
# 
# # -------------------- IBGE robusto --------------------
# def _strip_accents(s: str) -> str:
#     if not isinstance(s, str):
#         s = str(s)
#     return ''.join(ch for ch in unicodedata.normalize('NFKD', s) if not unicodedata.combining(ch))
# 
# def _normalize_cols(df: pd.DataFrame) -> pd.DataFrame:
#     cols = []
#     for c in df.columns:
#         base = _strip_accents(str(c)).lower().strip().replace(" ", "_")
#         base = "".join(ch if ch.isalnum() or ch == "_" else "_" for ch in base)
#         cols.append(base)
#     out = df.copy()
#     out.columns = cols
#     return out
# 
# def try_merge_ibge(df, xls_path):
#     if not xls_path or not os.path.exists(xls_path):
#         print("[INFO] sem IBGE XLS/XLSX â€” pulando")
#         return df
# 
#     ext = os.path.splitext(xls_path)[1].lower()
#     engine = "openpyxl" if ext == ".xlsx" else "xlrd"
# 
#     try:
#         x = pd.read_excel(xls_path, header=6, engine=engine)
#     except Exception:
#         x = pd.read_excel(xls_path, header=0, engine=engine)
# 
#     x = _normalize_cols(x)
# 
#     code_candidates = [
#         "codigo_municipio_completo","codigo_municipio","cod_municipio",
#         "codigo_municipio_ibge","codigo_ibge","codigo_mun","cod_ibge"
#     ]
#     name_candidates = [
#         "nome_municipio","nome_municipio_","nome_municipio_ibge","nome_do_municipio","nome"
#     ]
#     code_col = next((c for c in code_candidates if c in x.columns), None)
#     name_col = next((c for c in name_candidates if c in x.columns), None)
# 
#     if not code_col or not name_col:
#         try:
#             x2 = pd.read_excel(xls_path, header=None, engine=engine)
#             header_row = None
#             for i in range(min(20, len(x2))):
#                 row_vals = [_strip_accents(str(v)).lower().strip() for v in x2.iloc[i].values]
#                 if ("codigo municipio completo" in row_vals) or ("codigo_municipio_completo" in row_vals):
#                     header_row = i; break
#             if header_row is not None:
#                 x = pd.read_excel(xls_path, header=header_row, engine=engine)
#                 x = _normalize_cols(x)
#                 code_col = next((c for c in code_candidates if c in x.columns), None)
#                 name_col = next((c for c in name_candidates if c in x.columns), None)
#         except Exception:
#             pass
# 
#     if not code_col or not name_col:
#         print(f"[WARN] IBGE: nÃ£o encontrei colunas de cÃ³digo/nome. Colunas: {list(x.columns)[:10]}...")
#         return df
# 
#     x = x[[code_col, name_col]].dropna()
#     x.rename(columns={code_col:"codigo_municipio_completo", name_col:"nome_municipio_ibge"}, inplace=True)
#     x["codigo_municipio_completo"] = (
#         x["codigo_municipio_completo"].astype(str).str.replace(r"\.0$","",regex=True).str.zfill(7)
#     )
# 
#     before = len(df)
#     df_merged = df.merge(x, left_on="cod_mun", right_on="codigo_municipio_completo", how="left")
#     df_merged["nome_municipio"] = df_merged.get("nome_municipio", df_merged["cod_mun"])
#     df_merged["nome_municipio"] = df_merged["nome_municipio_ibge"].fillna(df_merged["nome_municipio"])
#     matched = df_merged["nome_municipio_ibge"].notna().sum()
#     print(f"[INFO] IBGE merge: {matched}/{before} cÃ³digos casados ({matched/before:.1%}).")
# 
#     return df_merged.drop(columns=["codigo_municipio_completo","nome_municipio_ibge"], errors="ignore")
# 
# # -------------------- Pipeline frequente --------------------
# def build_classifier_pipeline():
#     feat_cols = ["ano_pib","valor_log","percentual"]
#     preproc = ColumnTransformer([
#         ("num", Pipeline([
#             ("imp", SimpleImputer(strategy="median")),
#             ("scaler", StandardScaler())
#         ]), feat_cols)
#     ], remainder="drop")
#     clf = LogisticRegression(multi_class="multinomial", class_weight="balanced",
#                              max_iter=1000, solver="lbfgs")
#     return Pipeline([("pre", preproc), ("clf", clf)])
# 
# def prepare_long_for_classification(long_df):
#     dfc = long_df.copy()
#     dfc["valor"] = pd.to_numeric(dfc["valor"], errors="coerce").clip(lower=0)
#     dfc["percentual"] = pd.to_numeric(dfc["percentual"], errors="coerce")
#     dfc["valor_log"] = np.log1p(dfc["valor"])
#     dfc = dfc.replace([np.inf, -np.inf], np.nan)
#     X = dfc[["ano_pib","valor_log","percentual"]]
#     y = dfc["setor"].astype(str)
#     return X, y, dfc
# 
# # -------------------- Bayes (multiclasse, softmax + ADVI) --------------------
# def fit_bayes_softmax(preprocessor, Xtr, ytr_str, Xte, ci=0.90, draws=1000, advi_steps=15000):
#     try:
#         import pymc as pm
#         try:
#             import pytensor.tensor as pt  # PyMC 5
#         except Exception:
#             import aesara.tensor as pt   # fallback PyMC 4
#         import arviz as az
#     except Exception as e:
#         return False, f"PyMC indisponÃ­vel: {e}", None, None, None, None
# 
#     Xtr_p = preprocessor.transform(Xtr)
#     Xte_p = preprocessor.transform(Xte)
#     classes = np.array(sorted(pd.Series(ytr_str).unique()))
#     K = len(classes)
#     if K < 3:
#         return False, "Softmax bayesiano: requer K>=3 classes.", None, None, None, None
# 
#     y_idx = pd.Series(ytr_str).map({c:i for i,c in enumerate(classes)}).values.astype("int64")
#     nfeat = Xtr_p.shape[1]
#     alpha = (1.0 - ci)/2.0
#     p_lo = 100*alpha
#     p_hi = 100*(1-alpha)
# 
#     try:
#         with pm.Model() as model:
#             # priors CONSERVADORES
#             B  = pm.Normal("B",  mu=0.0, sigma=1.0, shape=(nfeat, K-1))
#             b0 = pm.Normal("b0", mu=0.0, sigma=1.0, shape=(K-1,))
#             eta_km1 = pt.dot(Xtr_p, B) + b0
#             eta = pt.concatenate([eta_km1, pt.zeros((Xtr_p.shape[0],1))], axis=1)
#             p_tr = pm.Deterministic("p_tr", pm.math.softmax(eta))
#             pm.Categorical("y_obs", p=p_tr, observed=y_idx)
# 
#             approx = pm.fit(
#                 n=advi_steps,
#                 method="advi",
#                 obj_optimizer=pm.adam(learning_rate=1e-3),
#                 callbacks=[pm.callbacks.CheckParametersConvergence(tolerance=1e-2, diff='absolute')],
#                 progressbar=False
#             )
#             idata = approx.sample(draws=draws)
# 
#         B_s  = np.asarray(idata.posterior["B"]).reshape(-1, nfeat, K-1)
#         b0_s = np.asarray(idata.posterior["b0"]).reshape(-1, K-1)
#         S = B_s.shape[0]
#         logits_km1 = np.einsum("sfk,nf->snk", B_s, Xte_p) + b0_s[:,None,:]
#         logits = np.concatenate([logits_km1, np.zeros((S, Xte_p.shape[0], 1))], axis=2)
#         ex = np.exp(logits - logits.max(axis=2, keepdims=True))
#         probs = ex / ex.sum(axis=2, keepdims=True)   # [S, N, K]
# 
#         proba_mean = probs.mean(axis=0)
#         proba_lo   = np.percentile(probs, p_lo, axis=0)
#         proba_hi   = np.percentile(probs, p_hi, axis=0)
#         return True, None, classes, proba_mean, proba_lo, proba_hi
#     except Exception as e:
#         return False, f"Falha PyMC (ADVI): {e}", None, None, None, None
# 
# # -------------------- Main --------------------
# def main():
#     argp = argparse.ArgumentParser(description="ClassificaÃ§Ã£o de SETOR (frequente + bayesiana) com IC.")
#     # DB
#     argp.add_argument("--db_host", default=None); argp.add_argument("--db_port", default=None)
#     argp.add_argument("--db_name", default=None); argp.add_argument("--db_user", default=None)
#     argp.add_argument("--db_pwd",  default=None); argp.add_argument("--db_schema", default=None)
#     argp.add_argument("--table", default="public.pib_municipios")
#     # I/O
#     argp.add_argument("--out_dir", default="/content/drive/MyDrive/PIB_Forecast")
#     argp.add_argument("--ibge_xls_path", default="/content/drive/MyDrive/Bases de Dados/RELATORIO_DTB_BRASIL_2024_MUNICIPIOS.xls")
#     # split
#     argp.add_argument("--cutoff_year", type=int, default=None)
#     # bayes (multiclasse)
#     argp.add_argument("--use_bayes", action="store_true")
#     argp.add_argument("--bayes_draws", type=int, default=1000)
#     argp.add_argument("--advi_steps", type=int, default=15000)
#     argp.add_argument("--ci", type=float, default=0.90)
#     args, _ = argp.parse_known_args()
# 
#     engine = get_engine_from_env(args)
#     out_dir = _ensure_dir(args.out_dir)
# 
#     print("[INFO] Carregando tabela..."); df_raw = load_raw_table(engine, args.table)
#     print("[INFO] Normalizando chaves..."); df_norm = normalize_keys(df_raw)
#     df_norm = try_merge_ibge(df_norm, args.ibge_xls_path)
#     print("[INFO] Melt & percentuais..."); df_long = melt_setores(df_norm)
# 
#     # split temporal
#     if args.cutoff_year is None:
#         anos_sorted = np.sort(df_long["ano_pib"].unique())
#         if len(anos_sorted) < 2:
#             raise ValueError("Poucos anos")
#         cutoff = int(anos_sorted[-2])
#     else:
#         cutoff = int(args.cutoff_year)
# 
#     df_train = df_long[df_long["ano_pib"] <= cutoff].copy()
#     df_test  = df_long[df_long["ano_pib"] >  cutoff].copy()
#     if df_train.empty or df_test.empty:
#         raise ValueError("Split vazio; ajuste --cutoff_year")
# 
#     Xtr, ytr, _   = prepare_long_for_classification(df_train)
#     Xte, yte, dte = prepare_long_for_classification(df_test)
# 
#     # frequente
#     print("[INFO] Treinando FREQUENTISTA...")
#     pipe = build_classifier_pipeline().fit(Xtr, ytr)
#     preproc = pipe.named_steps["pre"]
#     ypred_freq = pipe.predict(Xte)
#     yproba_freq = pipe.predict_proba(Xte)
#     labels_freq = pipe.classes_
#     acc_freq = accuracy_score(yte, ypred_freq)
#     f1m_freq = f1_score(yte, ypred_freq, average="macro")
#     cm_freq = confusion_matrix(yte, ypred_freq, labels=labels_freq)
# 
#     # bayes (multiclasse)
#     bayes_ok=False; acc_bayes=f1m_bayes=None
#     if args.use_bayes:
#         print("[INFO] Treinando BAYESIANO (multiclasse, softmax+ADVI)...")
#         ok_flag, msg_bayes, classes_b, p_mean_b, p_lo_b, p_hi_b = fit_bayes_softmax(
#             preprocessor=preproc, Xtr=Xtr, ytr_str=ytr, Xte=Xte,
#             ci=args.ci, draws=args.bayes_draws, advi_steps=args.advi_steps
#         )
#         if not ok_flag:
#             print(f"[WARN] Bayes falhou: {msg_bayes}")
#         else:
#             bayes_ok=True
#             ypred_bayes = [classes_b[i] for i in p_mean_b.argmax(axis=1)]
#             acc_bayes = accuracy_score(yte, ypred_bayes)
#             f1m_bayes = f1_score(yte, ypred_bayes, average="macro")
#             cm_bayes = confusion_matrix(yte, ypred_bayes, labels=classes_b)
#             pd.DataFrame(cm_bayes, index=classes_b, columns=classes_b)\
#               .to_csv(os.path.join(out_dir, "metrics_classify_confusion_matrix_bayes.csv"), encoding="utf-8")
# 
#     # resultados
#     df_res = dte[["ano_pib","cod_mun","nome_municipio","setor","valor","percentual"]].copy()
#     df_res["pred_setor_freq"] = ypred_freq
#     df_res["proba_max_freq"]  = yproba_freq.max(axis=1)
#     if bayes_ok:
#         for j,cname in enumerate(classes_b):
#             df_res[f"p_bayes_mean__{cname}"] = p_mean_b[:,j]
#             df_res[f"p_bayes_lo__{cname}"]   = p_lo_b[:,j]
#             df_res[f"p_bayes_hi__{cname}"]   = p_hi_b[:,j]
#         idx_best = p_mean_b.argmax(axis=1)
#         df_res["pred_setor_bayes"] = [classes_b[i] for i in idx_best]
#         df_res["proba_max_bayes"]  = p_mean_b.max(axis=1)
#         df_res["proba_max_bayes_lo"] = p_lo_b[np.arange(len(idx_best)), idx_best]
#         df_res["proba_max_bayes_hi"] = p_hi_b[np.arange(len(idx_best)), idx_best]
# 
#     df_res.to_csv(os.path.join(out_dir, "results_classify_test.csv"), index=False, encoding="utf-8")
# 
#     # mÃ©tricas & confusÃµes
#     pd.DataFrame([{
#         "cutoff": cutoff,
#         "accuracy_freq": acc_freq, "f1_macro_freq": f1m_freq,
#         **({ "accuracy_bayes": acc_bayes, "f1_macro_bayes": f1m_bayes } if bayes_ok else {})
#     }]).to_csv(os.path.join(out_dir, "metrics_classify_summary.csv"), index=False, encoding="utf-8")
# 
#     pd.DataFrame(cm_freq, index=labels_freq, columns=labels_freq)\
#       .to_csv(os.path.join(out_dir, "metrics_classify_confusion_matrix_freq.csv"), encoding="utf-8")
# 
#     print(f"[OK] FREQ  -> acc={acc_freq:.3f} | f1={f1m_freq:.3f}")
#     if bayes_ok:
#         print(f"[OK] BAYES -> acc={acc_bayes:.3f} | f1={f1m_bayes:.3f}")
#     else:
#         print("[INFO] Bayes nÃ£o rodou (sem --use_bayes ou PyMC indisponÃ­vel).")
# 
# if __name__ == "__main__":
#     main()
#

df.columns

# ==== Instalar deps ====
!pip -q install -U streamlit==1.* pyngrok==7.* >/dev/null

# 1) instalar (se ainda nÃ£o tiver)
!pip -q install streamlit==1.* pyngrok==7.* plotly==5.*

# 2) subir o app localmente (porta 8501)
import sys, subprocess, os
os.environ["STREAMLIT_SERVER_HEADLESS"]="true"
proc = subprocess.Popen([sys.executable, "-m", "streamlit", "run",
                         "/content/drive/MyDrive/PIB_Forecast/streamlit_previsoes.py",
                         "--server.address", "0.0.0.0", "--server.port", "8501"])
print("âœ… Streamlit subindo em http://127.0.0.1:8501 (ou `Public URL` do Colab)")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /content/drive/MyDrive/PIB_Forecast/streamlit_previsoes.py
# # -*- coding: utf-8 -*-
# from pathlib import Path
# import numpy as np
# import pandas as pd
# import streamlit as st
# import plotly.express as px
# import plotly.graph_objects as go
# 
# # ====================== CONFIG BÃSICO ======================
# st.set_page_config(page_title="ExploraÃ§Ã£o & PrevisÃµes â€” PIB Setor", layout="wide")
# BASE_DIR = Path("/content/drive/MyDrive/PIB_Forecast")
# PLOTLY_TEMPLATE = "plotly_white"
# 
# # ====================== CSS ======================
# st.markdown("""
# <style>
# html, body, [class*="css"]{font-family:'Inter',system-ui,-apple-system,Segoe UI,Roboto,sans-serif}
# .section{border-radius:18px;border:1px solid rgba(0,0,0,.06);padding:18px;background:#fff;box-shadow:0 8px 24px rgba(0,0,0,.04)}
# .metric-card{border-radius:16px;padding:16px 18px;border:1px solid rgba(0,0,0,.06);background:linear-gradient(180deg,rgba(0,0,0,.03),rgba(0,0,0,.015));box-shadow:0 4px 12px rgba(0,0,0,.05)}
# .small{color:#666;font-size:.9rem}
# h1,h2,h3{letter-spacing:.2px}
# hr{margin:.6rem 0 1rem 0;border-color:rgba(0,0,0,.08)}
# </style>
# """, unsafe_allow_html=True)
# 
# # ====================== HELPERS ======================
# @st.cache_data(show_spinner=False)
# def _read_csv(p: Path) -> pd.DataFrame | None:
#     try:
#         if not p.exists(): return None
#         try:
#             df = pd.read_csv(p, dtype={"ano_pib": "Int64"})
#         except Exception:
#             df = pd.read_csv(p)
#         if "ano_pib" in df.columns:
#             df["ano_pib"] = pd.to_numeric(df["ano_pib"], errors="coerce").astype("Int64")
#         return df
#     except Exception as e:
#         st.warning(f"Falha ao ler **{p.name}**: {e}")
#         return None
# 
# @st.cache_data(show_spinner=False)
# def _read_cm(p: Path) -> pd.DataFrame | None:
#     try:
#         if not p.exists(): return None
#         df = pd.read_csv(p, index_col=0)
#         df.index = df.index.astype(str); df.columns = df.columns.astype(str)
#         if list(df.index) != list(df.columns):
#             labs = list(dict.fromkeys(list(df.index)+list(df.columns)))
#             df = df.reindex(index=labs, columns=labs, fill_value=0)
#         return df
#     except Exception as e:
#         st.warning(f"Falha ao ler **{p.name}**: {e}")
#         return None
# 
# @st.cache_data(show_spinner=False)
# def _read_txt(p: Path) -> str | None:
#     try:
#         if p.exists(): return p.read_text(encoding="utf-8")
#     except Exception as e:
#         st.warning(f"Falha ao ler **{p.name}**: {e}")
#     return None
# 
# def _download(df: pd.DataFrame, label: str, filename: str):
#     st.download_button(label, df.to_csv(index=False).encode("utf-8"),
#                        file_name=filename, mime="text/csv", use_container_width=True)
# 
# def _normalize_cm(cm: pd.DataFrame, how: str) -> pd.DataFrame:
#     if how == "Contagem": return cm
#     A = cm.values.astype(float)
#     if how == "% por linha (verdadeiro)":
#         row = A.sum(axis=1, keepdims=True); row[row==0]=1; A = 100*A/row
#     elif how == "% por coluna (predito)":
#         col = A.sum(axis=0, keepdims=True); col[col==0]=1; A = 100*A/col
#     elif how == "% do total":
#         s = A.sum() or 1; A = 100*A/s
#     return pd.DataFrame(A, index=cm.index, columns=cm.columns).round(2)
# 
# def _anos_unicos_as_str(df: pd.DataFrame) -> list[str]:
#     if "ano_pib" not in df.columns: return []
#     anos = pd.to_numeric(df["ano_pib"], errors="coerce").dropna().astype(int).unique().tolist()
#     return [str(a) for a in sorted(anos)]
# 
# def _aplicar_filtro_ano(df: pd.DataFrame, ano_str: str) -> pd.DataFrame:
#     if ano_str == "(todos)" or "ano_pib" not in df.columns: return df
#     ano_int = int(ano_str)
#     return df[pd.to_numeric(df["ano_pib"], errors="coerce").astype("Int64") == ano_int]
# 
# def _count_df(series: pd.Series, label_name="setor") -> pd.DataFrame:
#     """Conserta duplicidade de nomes apÃ³s value_counts()."""
#     s = series.dropna().astype(str).value_counts()
#     return s.rename_axis(label_name).reset_index(name="count")
# 
# # ====================== ARQUIVOS ======================
# RES_BAYES = BASE_DIR / "results_classify_test_BAYES.csv"
# SUM_BAYES = BASE_DIR / "metrics_classify_summary_BAYES.csv"
# CM_BAYES  = BASE_DIR / "metrics_classify_confusion_matrix_BAYES.csv"
# TXT_BAYES = BASE_DIR / "classification_report_BAYES.txt"
# RES_FREQ  = BASE_DIR / "results_classify_test.csv"
# SUM_FREQ  = BASE_DIR / "metrics_classify_summary.csv"
# CM_FREQ   = BASE_DIR / "metrics_classify_confusion_matrix_freq.csv"
# 
# df_res_bayes = _read_csv(RES_BAYES)
# df_sum_bayes = _read_csv(SUM_BAYES)
# df_cm_bayes  = _read_cm(CM_BAYES)
# txt_bayes    = _read_txt(TXT_BAYES)
# df_res_freq  = _read_csv(RES_FREQ)
# df_sum_freq  = _read_csv(SUM_FREQ)
# df_cm_freq   = _read_cm(CM_FREQ)
# 
# # ====================== HEADER ======================
# st.markdown("<h1>ðŸ”Ž ExploraÃ§Ã£o & ðŸŽ¯ PrevisÃµes â€” PIB por Setor (Interativo)</h1>", unsafe_allow_html=True)
# with st.expander("Arquivos detectados", expanded=False):
#     c1, c2 = st.columns(2)
#     c1.write(f"**Bayes**: "
#              f"{'âœ”ï¸' if df_res_bayes is not None else 'âŒ'} results | "
#              f"{'âœ”ï¸' if df_sum_bayes is not None else 'âŒ'} summary | "
#              f"{'âœ”ï¸' if df_cm_bayes is not None else 'âŒ'} confusion")
#     c2.write(f"**Frequente**: "
#              f"{'âœ”ï¸' if df_res_freq is not None else 'âŒ'} results | "
#              f"{'âœ”ï¸' if df_sum_freq is not None else 'âŒ'} summary | "
#              f"{'âœ”ï¸' if df_cm_freq is not None else 'âŒ'} confusion")
# 
# # ====================== SIDEBAR ======================
# st.sidebar.header("Fonte dos resultados")
# opts = []
# if df_res_bayes is not None: opts.append("Bayes (ADVI)")
# if df_res_freq  is not None: opts.append("Frequente (Logistic)")
# if not opts:
#     st.error("Nenhum resultado encontrado em /content/drive/MyDrive/PIB_Forecast.")
#     st.stop()
# src_choice = st.sidebar.radio("Modelo:", opts, index=0)
# 
# df_base = (df_res_bayes.copy() if src_choice.startswith("Bayes") else df_res_freq.copy())
# st.sidebar.subheader("Filtros globais")
# anos_str = _anos_unicos_as_str(df_base)
# ano_sel = st.sidebar.selectbox("Ano", ["(todos)"]+anos_str, index=0)
# q_muni  = st.sidebar.text_input("Buscar municÃ­pio (nome ou cÃ³digo)")
# setores = sorted(df_base["setor"].dropna().unique().tolist()) if "setor" in df_base.columns else []
# set_true = st.sidebar.selectbox("Setor verdadeiro", ["(todos)"]+setores, index=0)
# 
# df_filtered = _aplicar_filtro_ano(df_base, ano_sel)
# if q_muni.strip():
#     q = q_muni.lower().strip()
#     df_filtered = df_filtered[
#         (df_filtered["nome_municipio"].astype(str).str.lower().str.contains(q)) |
#         (df_filtered["cod_mun"].astype(str).str.contains(q))
#     ]
# if set_true != "(todos)" and "setor" in df_filtered.columns:
#     df_filtered = df_filtered[df_filtered["setor"] == set_true]
# 
# # ====================== TABS ======================
# tab_exp, tab_prev = st.tabs(["âœ¨ AnÃ¡lises ExploratÃ³rias", "ðŸŽ¯ PrevisÃµes"])
# 
# # ====================== EXPLORATÃ“RIA ======================
# with tab_exp:
#     if df_filtered.empty:
#         st.info("Sem dados para os filtros atuais."); st.stop()
# 
#     left, right = st.columns(2, gap="large")
# 
#     # -------- Esquerda: visÃ£o geral + evoluÃ§Ã£o --------
#     with left:
#         st.markdown('<div class="section">', unsafe_allow_html=True)
#         st.subheader("VisÃ£o geral")
#         m1, m2, m3, m4 = st.columns(4)
#         m1.markdown(f'<div class="metric-card"><div class="small">ObservaÃ§Ãµes</div><h3>{len(df_filtered):,}</h3></div>', unsafe_allow_html=True)
#         m2.markdown(f'<div class="metric-card"><div class="small">MunicÃ­pios</div><h3>{df_filtered["cod_mun"].nunique():,}</h3></div>', unsafe_allow_html=True)
#         m3.markdown(f'<div class="metric-card"><div class="small">Anos</div><h3>{df_filtered["ano_pib"].nunique()}</h3></div>', unsafe_allow_html=True)
#         m4.markdown(f'<div class="metric-card"><div class="small">Setores</div><h3>{df_filtered["setor"].nunique()}</h3></div>', unsafe_allow_html=True)
# 
#         st.markdown("<hr/>", unsafe_allow_html=True)
#         order_by = st.selectbox("Ordenar barras por:", ["Contagem desc.", "AlfabÃ©tica"], index=0)
# 
#         df_bar = _count_df(df_filtered["setor"])
#         df_bar = df_bar.sort_values(("setor" if order_by=="AlfabÃ©tica" else "count"),
#                                     ascending=False if order_by!="AlfabÃ©tica" else True)
# 
#         fig = px.bar(df_bar, x="setor", y="count",
#                      labels={"setor":"Setor","count":"Qtde"},
#                      title="Contagem por setor (verdadeiro)",
#                      template=PLOTLY_TEMPLATE)
#         fig.update_traces(hovertemplate="<b>%{x}</b><br>Qtde: %{y:,}")
#         fig.update_layout(xaxis_tickangle=25)
#         st.plotly_chart(fig, use_container_width=True)
#         st.markdown('</div>', unsafe_allow_html=True)
# 
#         st.markdown('<div class="section">', unsafe_allow_html=True)
#         st.subheader("EvoluÃ§Ã£o temporal â€” quantidade por setor")
#         stacked = st.checkbox("Empilhar linhas por setor (Ã¡rea empilhada)", value=False)
#         tmp = (df_filtered.assign(
#                 ano_pib=pd.to_numeric(df_filtered["ano_pib"], errors="coerce").astype("Int64"))
#                .dropna(subset=["ano_pib"]))
#         tmp["ano_pib"] = tmp["ano_pib"].astype(int)
#         tmp = (tmp.groupby(["ano_pib","setor"]).size()
#                .reset_index(name="qtd").sort_values(["ano_pib","setor"]))
#         fig2 = (px.area if stacked else px.line)(
#             tmp, x="ano_pib", y="qtd", color="setor",
#             template=PLOTLY_TEMPLATE, labels={"ano_pib":"Ano","qtd":"Qtde","setor":"Setor"},
#             **({"markers": True} if not stacked else {})
#         )
#         fig2.update_traces(hovertemplate="<b>%{fullData.name}</b><br>Ano: %{x}<br>Qtde: %{y:,}")
#         st.plotly_chart(fig2, use_container_width=True)
#         _download(df_filtered, "Baixar recorte (CSV)",
#                   f"exploratorio_{'bayes' if src_choice.startswith('Bayes') else 'freq'}.csv")
#         st.markdown('</div>', unsafe_allow_html=True)
# 
#     # -------- Direita: prediÃ§Ãµes + confianÃ§a --------
#     with right:
#         st.markdown('<div class="section">', unsafe_allow_html=True)
#         if src_choice.startswith("Bayes") and "pred_setor_bayes" in df_filtered.columns:
#             st.subheader("PrediÃ§Ãµes (Bayes)")
#             df_pred_bar = _count_df(df_filtered["pred_setor_bayes"]).sort_values("count", ascending=False)
#             fig3 = px.bar(df_pred_bar, x="setor", y="count",
#                           labels={"setor":"Setor","count":"Qtde"},
#                           title="Contagem por setor (predito â€” Bayes)", template=PLOTLY_TEMPLATE)
#             fig3.update_traces(hovertemplate="<b>%{x}</b><br>Qtde: %{y:,}")
#             fig3.update_layout(xaxis_tickangle=25)
#             st.plotly_chart(fig3, use_container_width=True)
# 
#             if "proba_max_bayes" in df_filtered.columns:
#                 conf = (df_filtered.groupby("pred_setor_bayes")["proba_max_bayes"]
#                         .mean().sort_values(ascending=False)).reset_index()
#                 conf = conf.rename(columns={"pred_setor_bayes":"setor","proba_max_bayes":"proba"})
#                 fig4 = px.bar(conf, x="setor", y="proba",
#                               labels={"setor":"Setor","proba":"Proba mÃ©dia"},
#                               title="Probabilidade mÃ¡xima mÃ©dia por setor (Bayes)",
#                               template=PLOTLY_TEMPLATE)
#                 fig4.update_traces(hovertemplate="<b>%{x}</b><br>Proba mÃ©dia: %{y:.3f}")
#                 fig4.update_layout(xaxis_tickangle=25, yaxis_range=[0,1])
#                 st.plotly_chart(fig4, use_container_width=True)
# 
#             st.markdown("##### Top municÃ­pios por confianÃ§a (Bayes)")
#             topk = st.slider("Top K", 5, 50, 10, 1, key="k_b")
#             cols = ["ano_pib","cod_mun","nome_municipio","setor","pred_setor_bayes","proba_max_bayes"]
#             st.dataframe(df_filtered.sort_values("proba_max_bayes", ascending=False).head(topk)[cols],
#                          hide_index=True, use_container_width=True)
# 
#         else:
#             st.subheader("PrediÃ§Ãµes (Frequente)")
#             df_pred_bar = _count_df(df_filtered["pred_setor_freq"]).sort_values("count", ascending=False)
#             fig5 = px.bar(df_pred_bar, x="setor", y="count",
#                           labels={"setor":"Setor","count":"Qtde"},
#                           title="Contagem por setor (predito â€” Freq.)", template=PLOTLY_TEMPLATE)
#             fig5.update_traces(hovertemplate="<b>%{x}</b><br>Qtde: %{y:,}")
#             fig5.update_layout(xaxis_tickangle=25)
#             st.plotly_chart(fig5, use_container_width=True)
# 
#             if "proba_max_freq" in df_filtered.columns:
#                 conf = (df_filtered.groupby("pred_setor_freq")["proba_max_freq"]
#                         .mean().sort_values(ascending=False)).reset_index()
#                 conf = conf.rename(columns={"pred_setor_freq":"setor","proba_max_freq":"proba"})
#                 fig6 = px.bar(conf, x="setor", y="proba",
#                               labels={"setor":"Setor","proba":"Proba mÃ©dia"},
#                               title="Probabilidade mÃ¡xima mÃ©dia por setor (Freq.)",
#                               template=PLOTLY_TEMPLATE)
#                 fig6.update_traces(hovertemplate="<b>%{x}</b><br>Proba mÃ©dia: %{y:.3f}")
#                 fig6.update_layout(xaxis_tickangle=25, yaxis_range=[0,1])
#                 st.plotly_chart(fig6, use_container_width=True)
# 
#             st.markdown("##### Top municÃ­pios por confianÃ§a (Freq.)")
#             topk = st.slider("Top K", 5, 50, 10, 1, key="k_f")
#             cols = ["ano_pib","cod_mun","nome_municipio","setor","pred_setor_freq","proba_max_freq"]
#             st.dataframe(df_filtered.sort_values("proba_max_freq", ascending=False).head(topk)[cols],
#                          hide_index=True, use_container_width=True)
#         st.markdown('</div>', unsafe_allow_html=True)
# 
# # ====================== PREVISÃ•ES ======================
# with tab_prev:
#     left, right = st.columns(2, gap="large")
# 
#     with left:
#         st.markdown('<div class="section">', unsafe_allow_html=True)
#         st.subheader("MÃ©tricas & RelatÃ³rios")
# 
#         if src_choice.startswith("Bayes"):
#             if df_sum_bayes is not None and len(df_sum_bayes)>0:
#                 r = df_sum_bayes.iloc[0]
#                 c1,c2,c3 = st.columns(3)
#                 c1.markdown(f'<div class="metric-card"><div class="small">Cutoff</div><h3>{int(r.get("cutoff", np.nan))}</h3></div>', unsafe_allow_html=True)
#                 c2.markdown(f'<div class="metric-card"><div class="small">Accuracy</div><h3>{float(r.get("accuracy_bayes", np.nan)):.3f}</h3></div>', unsafe_allow_html=True)
#                 c3.markdown(f'<div class="metric-card"><div class="small">F1 Macro</div><h3>{float(r.get("f1_macro_bayes", np.nan)):.3f}</h3></div>', unsafe_allow_html=True)
#                 _download(df_sum_bayes, "Baixar mÃ©tricas (CSV)", "metrics_classify_summary_BAYES.csv")
#             else:
#                 st.info("metrics_classify_summary_BAYES.csv nÃ£o disponÃ­vel.")
# 
#             st.markdown("<hr/>", unsafe_allow_html=True)
#             st.subheader("Matriz de confusÃ£o (Bayes)")
#             if df_cm_bayes is not None and not df_cm_bayes.empty:
#                 norm = st.selectbox("NormalizaÃ§Ã£o", ["Contagem","% por linha (verdadeiro)","% por coluna (predito)","% do total"], 0, key="norm_b")
#                 cm_show = _normalize_cm(df_cm_bayes, norm)
#                 ztext = np.vectorize(lambda x: f"{x:.0f}" if norm=="Contagem" else f"{x:.1f}%")(cm_show.values)
#                 fig = go.Figure(data=go.Heatmap(
#                     z=cm_show.values, x=cm_show.columns, y=cm_show.index,
#                     colorscale="Blues", colorbar_title=("Qtde" if norm=="Contagem" else "%"),
#                     text=ztext, texttemplate="%{text}",
#                     hovertemplate="Verdadeiro=%{y}<br>Predito=%{x}<br>Valor=%{z}<extra></extra>"
#                 ))
#                 fig.update_layout(template=PLOTLY_TEMPLATE, margin=dict(l=10,r=10,t=40,b=10),
#                                   title=f"Bayes â€” Confusion Matrix ({norm})")
#                 st.plotly_chart(fig, use_container_width=True)
#                 _download(cm_show.reset_index().rename(columns={"index":"__label__"}),
#                           "Baixar matriz (CSV)", "metrics_classify_confusion_matrix_BAYES_view.csv")
#             else:
#                 st.info("metrics_classify_confusion_matrix_BAYES.csv nÃ£o disponÃ­vel.")
# 
#             st.markdown("<hr/>", unsafe_allow_html=True)
#             st.subheader("RelatÃ³rio de classificaÃ§Ã£o (txt)")
#             st.code(txt_bayes or "RelatÃ³rio nÃ£o encontrado.", language="text")
# 
#         else:
#             if df_sum_freq is not None and len(df_sum_freq)>0:
#                 r = df_sum_freq.iloc[0]
#                 c1,c2,c3 = st.columns(3)
#                 c1.markdown(f'<div class="metric-card"><div class="small">Cutoff</div><h3>{int(r.get("cutoff", np.nan))}</h3></div>', unsafe_allow_html=True)
#                 c2.markdown(f'<div class="metric-card"><div class="small">Accuracy</div><h3>{float(r.get("accuracy_freq", np.nan)):.3f}</h3></div>', unsafe_allow_html=True)
#                 c3.markdown(f'<div class="metric-card"><div class="small">F1 Macro</div><h3>{float(r.get("f1_macro_freq", np.nan)):.3f}</h3></div>', unsafe_allow_html=True)
#                 _download(df_sum_freq, "Baixar mÃ©tricas (CSV)", "metrics_classify_summary.csv")
#             else:
#                 st.info("metrics_classify_summary.csv nÃ£o disponÃ­vel.")
# 
#             st.markdown("<hr/>", unsafe_allow_html=True)
#             st.subheader("Matriz de confusÃ£o (Frequente)")
#             if df_cm_freq is not None and not df_cm_freq.empty:
#                 norm = st.selectbox("NormalizaÃ§Ã£o", ["Contagem","% por linha (verdadeiro)","% por coluna (predito)","% do total"], 0, key="norm_f")
#                 cm_show = _normalize_cm(df_cm_freq, norm)
#                 ztext = np.vectorize(lambda x: f"{x:.0f}" if norm=="Contagem" else f"{x:.1f}%")(cm_show.values)
#                 fig = go.Figure(data=go.Heatmap(
#                     z=cm_show.values, x=cm_show.columns, y=cm_show.index,
#                     colorscale="Blues", colorbar_title=("Qtde" if norm=="Contagem" else "%"),
#                     text=ztext, texttemplate="%{text}",
#                     hovertemplate="Verdadeiro=%{y}<br>Predito=%{x}<br>Valor=%{z}<extra></extra>"
#                 ))
#                 fig.update_layout(template=PLOTLY_TEMPLATE, margin=dict(l=10,r=10,t=40,b=10),
#                                   title=f"Frequente â€” Confusion Matrix ({norm})")
#                 st.plotly_chart(fig, use_container_width=True)
#                 _download(cm_show.reset_index().rename(columns={"index":"__label__"}),
#                           "Baixar matriz (CSV)", "metrics_classify_confusion_matrix_freq_view.csv")
#             else:
#                 st.info("metrics_classify_confusion_matrix_freq.csv nÃ£o disponÃ­vel.")
#         st.markdown('</div>', unsafe_allow_html=True)
# 
#     with right:
#         st.markdown('<div class="section">', unsafe_allow_html=True)
#         st.subheader("PrevisÃµes por observaÃ§Ã£o (teste)")
# 
#         if src_choice.startswith("Bayes"):
#             dfshow = df_res_bayes.copy()
#             c1,c2,c3,c4 = st.columns(4)
#             anos_local = _anos_unicos_as_str(dfshow)
#             ano  = c1.selectbox("Ano", ["(todos)"]+anos_local, 0, key="b_ano")
#             muni = c2.text_input("Filtro municÃ­pio", key="b_muni")
#             true_ = c3.selectbox("Setor verdadeiro", ["(todos)"]+sorted(dfshow["setor"].dropna().unique().tolist()), 0, key="b_true")
#             pred_ = c4.selectbox("Setor predito (Bayes)", ["(todos)"]+sorted(dfshow["pred_setor_bayes"].dropna().unique().tolist()), 0, key="b_pred")
# 
#             dfshow = _aplicar_filtro_ano(dfshow, ano)
#             if muni.strip():
#                 q = muni.lower().strip()
#                 dfshow = dfshow[(dfshow["nome_municipio"].astype(str).str.lower().str.contains(q)) |
#                                 (dfshow["cod_mun"].astype(str).str.contains(q))]
#             if true_ != "(todos)": dfshow = dfshow[dfshow["setor"] == true_]
#             if pred_ != "(todos)": dfshow = dfshow[dfshow["pred_setor_bayes"] == pred_]
# 
#             proba_cols = [c for c in dfshow.columns if c.startswith("p_bayes_mean__")]
#             cols = ["ano_pib","cod_mun","nome_municipio","setor","valor","percentual",
#                     "pred_setor_bayes","proba_max_bayes","proba_max_bayes_lo","proba_max_bayes_hi"] + proba_cols
#             st.dataframe(dfshow[cols], hide_index=True, use_container_width=True)
#             _download(dfshow, "Baixar previsÃµes (CSV)", "results_classify_test_BAYES_filtrado.csv")
# 
#         else:
#             dfshow = df_res_freq.copy()
#             c1,c2,c3,c4 = st.columns(4)
#             anos_local = _anos_unicos_as_str(dfshow)
#             ano  = c1.selectbox("Ano", ["(todos)"]+anos_local, 0, key="f_ano")
#             muni = c2.text_input("Filtro municÃ­pio", key="f_muni")
#             true_ = c3.selectbox("Setor verdadeiro", ["(todos)"]+sorted(dfshow["setor"].dropna().unique().tolist()), 0, key="f_true")
#             pred_ = c4.selectbox("Setor predito (Freq.)", ["(todos)"]+sorted(dfshow["pred_setor_freq"].dropna().unique().tolist()), 0, key="f_pred")
# 
#             dfshow = _aplicar_filtro_ano(dfshow, ano)
#             if muni.strip():
#                 q = muni.lower().strip()
#                 dfshow = dfshow[(dfshow["nome_municipio"].astype(str).str.lower().str.contains(q)) |
#                                 (dfshow["cod_mun"].astype(str).str.contains(q))]
#             if true_ != "(todos)": dfshow = dfshow[dfshow["setor"] == true_]
#             if pred_ != "(todos)": dfshow = dfshow[dfshow["pred_setor_freq"] == pred_]
# 
#             cols = ["ano_pib","cod_mun","nome_municipio","setor","valor","percentual",
#                     "pred_setor_freq","proba_max_freq"]
#             st.dataframe(dfshow[cols], hide_index=True, use_container_width=True)
#             _download(dfshow, "Baixar previsÃµes (CSV)", "results_classify_test_filtrado.csv")
#         st.markdown('</div>', unsafe_allow_html=True)
#